{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Regression_Week13.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vMc9bk8cjDB0"},"source":["#**Regression**\n","As we may remember, there are two major machine learning techniques under the supervised ML category: **regression** and **classification**. Both use 'labeled' data to build a model for prediction (usually). If the prediction is a real valued number, it falls under the category of *regression*. If the prediction is a categorical or discrete values, it's called *classification*. And to add some confusion, *logistic* regression is a type of *classification*.\n","\n","#**Linear Regression**\n","\n","Linear regression is one of those words (like calculus or linear algebra) where we have probably have heard of it before (perhaps in a high school algebra class, or a college statistics class), but may not *know* (or forgotten) what it does or how it works.\n","\n","Recently, it seems to have made a comeback and the fields of data science and machine learning are trying to claim 'ownership' of it. Linear regression is a technique that goes back to the early 1800's and is strongly rooted in the field of statistics.\n","\n","#**Linear vs Polynomial Fitting**\n","This lesson's focus is on regression, but specifically linear regression. Linear regression uses the equation of a line as its 'model'. Here a straight line is used to represent the data (the best it can).\n","\n","![](https://drive.google.com/uc?export=view&id=1jVlCo94YUV1rQooo_VTV6yfaKiaIqdk7)\n","\n","If the line needs to become a curve, it becomes *polynomial* regression. We will look at this later in the lesson.\n","\n","#**Single vs Multiple Independent Variables**\n","\n","When there is only one independent variable involved (also called the predictor, explanatory), it's called *simple* linear regression or *univariate* linear regression. If more than one independent variable is involved, it called *multiple (or multivariate)* linear regression. If regression is used without any qualifiers, it's usually simple, linear regression.\n","\n","In all possible configurations, mastering the simple linear regression model makes understanding all the others fall into place -- much of the techniques stay the same, just in 'higher' dimensions.\n","\n","#**A Simple Goal**\n","The goal of linear regression is to find the best fitting line for a set of points/objects in a dataset. Once you have this line, it can be used to make predictions (assuming the line does a good job of modeling the data).\n","\n","![](https://drive.google.com/uc?export=view&id=1fvWUvDW1QhyttAvVVMmqnFY8x7KKq8gN)\n","\n","\n","#**A Line, revisited**\n","As we saw in a previous lesson, a line is defined by its slope and y intercept. The equation of line $y = mx + b$ is the heart of linear regression.\n","\n","If you think of this equation as a function, it could be written as\n","```\n","def f(x):\n","  return m * x + b\n","```\n","\n","* $y$ is the output of the function (the dependent, response, outcome variable)\n","* $x$ is the input to the function (the independent, predictor, explanatory variable) \n","* $m$ represents the slope\n","* $b$ represents the $y$ intercept (the value when $x$ is zero)\n","\n","Note that this equation, $y = mx+b$ is sometime referred to as *polynomial* equation of degree 1 (i.e. first degree polynomial). If you had an equation that looked like $y = ax^2 + bx + c$, you would call it a quadratic equation. To solve this, you set $y$ to $0$ and use the quadratic equation to find the values of $x$. But we digress.\n","\n","There are different algorithms for linear regression (the focus of this lesson), but essentially, each is trying to determine or find a line's slope $(m)$ and its y-intercept $(b\\ or\\ y0)$."]},{"cell_type":"markdown","metadata":{"id":"WO6-UtfPkzyj"},"source":["#**A dataset of 2 Points**\n","\n","![](https://drive.google.com/uc?export=view&id=1jb_PkAhoL1Mw2T5lz94x6YGJoeM5Ng3n)\n","\n","The simplest line (as shown above), can be derived by 2 points. The resulting line 'perfectly fits' the data (the 2 points). Each point is on the calculated line; the distance from the line to each point is zero. If the dataset has a small number of samples, we need to use additional metrics if we want to make any claims with statistical confidence.\n","\n","#**A dataset of 3 Points (or more)**\n","\n","When you add additional points to the data, calculating the slope and y-intercept is no longer trivial. You can pick any two points from the dataset to create a line; however, there will be no line that passes through all the other points (unless they are all one the line -- that is, they are some linear combination of the two selected points).\n","\n","The goal of linear regression is to find the best *fitting line*. We define 'best fitting' as the line that minimizes the sum of the squared distances between the y values in the dataset (the actual values) and the y values calculated from the fitted line ŷ (y-hat).\n","\n","\n","![](https://drive.google.com/uc?export=view&id=1WzaI6D1LpqarHqZCFdyr_gtXWS0jqsrO)"]},{"cell_type":"markdown","metadata":{"id":"U7tvH6JGlUMO"},"source":["#**The Objective Function**\n","The sum of squared distances is called the sum of squared errors (SSE) or **residuals**. So the goal of linear regression is to minimize the following function (be sure to understand this, before moving on) for the n data observations:\n","\n","\n","<div align=\"center\"> $SSE = \\sum_{i=1}^n (y_i - \\hat{y_i})^2$ </div>\n","\n","\n","How do we do that? That is how can one select the line (defined by its slope and y-intercept) such that the SSE is minimized?\n","\n","Well, that **is** the field of linear regression. This lesson will look at different techniques to fit a line to a dataset that minimized the SSE and some ways to apply regression to a dataset. In math, statistics, and machine learning, they use different techniques to take the data and 'figure out' which line best models the data.\n","\n","Once we find the line that minimizes SSE, we can then calculate other statistics to help us understand the correlation (if any) between the data attributes (i.e. \"is there a linear relationship between x and y?\").\n","\n","#**A page out of Stats 101 (literally)**\n","\n","![](https://drive.google.com/uc?export=view&id=1BHCtqiIpk_CpYe058k09wqJFz-3d8Ag-)\n","\n","\n","#**A Statistician's View of Linear Regression**\n","The above image is taken from page 450 from Learning From Data ($2^{nd}$ edition). But any statistics book on linear regression will have the formulas necessary to find the slope and y- intercept of the regression line (the one that minimizes SSE).\n","\n","In this example, we are going to use the same dataset the book uses and go over the formulas (the dataset comes from a fictitious sociologist investigating the relationship between the number of years mothers attend college and the number of years their daughters attend college).\n","\n","![](https://drive.google.com/uc?export=view&id=15fJ0UbVmeoL74MmA80sCsXlgzgFFRPK1)\n","\n","Let's load up the data. The following function uses pandas to load the dataset, but then returns two parallel numpy arrays, one for the x (the independent variable) attribute and one for the y (the dependent variable) attribute."]},{"cell_type":"code","metadata":{"id":"rrWDCyC3mCB9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638301041910,"user_tz":360,"elapsed":128,"user":{"displayName":"Pratyush Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjdfJ0UzoDFRl5Z7hqJADmVKGbPH8sJ4KQ0IJsvg=s64","userId":"17344282814232432823"}},"outputId":"19222607-edbb-407b-857c-63ad17e1d6e8"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","def xy_from_file(p, x, y, show=False):\n","    df = pd.read_csv(p)\n","    if show:\n","        df['XY'] = df[x] * df[y]\n","        print(df.head())\n","    x_values = df[x].values\n","    y_values = df[y].values\n","    return x_values, y_values\n","xv, yv = xy_from_file(('data20-1.csv'), 'Mother', 'Daughter', show=True)"],"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["   ID  Mother  Daughter  XY\n","0   1       0         0   0\n","1   2       0         1   0\n","2   3       1         2   2\n","3   4       2         3   6\n","4   5       2         4   8\n"]}]},{"cell_type":"markdown","metadata":{"id":"QS-57EA1l1nS"},"source":["#**All those ∑'s**\n","\n","Looking at Table 20-1, we now need to calculate the various summation quantities $(e.g. \\sum x, \\sum y, \\sum x^2, \\sum y^2, etc)$. These will be used for determining the slope and y-intercept of a line. Luckily we can use numpy to make this very straightforward:"]},{"cell_type":"code","metadata":{"id":"ih-l9OZ1mFkZ","executionInfo":{"status":"ok","timestamp":1638301072517,"user_tz":360,"elapsed":127,"user":{"displayName":"Pratyush Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjdfJ0UzoDFRl5Z7hqJADmVKGbPH8sJ4KQ0IJsvg=s64","userId":"17344282814232432823"}}},"source":["def find_line(x,y):\n","  \n","    x_sum  = np.sum(x)              # ∑x\n","    y_sum  = np.sum(y)              # ∑y\n","    xy_sum = np.sum(x * y)          # ∑(xy)\n","\n","    x2_sum = np.sum(np.square(x))   # ∑(x2) \n","    y2_sum = np.sum(np.square(y))   # ∑(y2)\n","    \n","    x_ave  = np.mean(x)\n","    y_ave  = np.mean(y)\n","    n = np.size(x)\n","\n","    dx = (n * xy_sum - x_sum * y_sum)\n","    dy = (n * x2_sum - x_sum * x_sum)\n","    \n","    slope = dx/dy\n","    y0 = y_ave - slope * x_ave  # y0 is the same as b\n","    \n","    print(\"y = {:.3f}x + {:.3f}\".format(slope, y0))\n"],"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qv09rfoJmsAo"},"source":["#**Ready for plugin**\n","\n","![](https://drive.google.com/uc?export=view&id=1NiSSHm8YAUMsoEZAXJ7IhatP57bhhetJ)\n","\n","The above image gives all the necessary formulas for calculating the slope and y-intercept with the above calculations.\n","Add the following code to find_line.\n","\n","```\n","n = np.size(x)\n","dx = (n * xy_sum - x_sum * y_sum)\n","dy = (n * x2_sum - x_sum * x_sum)\n","\n","slope = dx/dy\n","y0 = y_ave - slope * x_ave  # y0 is the same as b\n","\n","print(\"y = {:.3f}x + {:.3f}\".format(slope, y0))\n","```\n","\n","The following should now work:\n","\n","```\n","xv, yv = xy_from_file(('data20-1.csv'), 'Mother', 'Daughter')\n","find_line(xv, yv)\n","```"]},{"cell_type":"code","metadata":{"id":"18qCUkz5m5Pf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638301075501,"user_tz":360,"elapsed":178,"user":{"displayName":"Pratyush Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjdfJ0UzoDFRl5Z7hqJADmVKGbPH8sJ4KQ0IJsvg=s64","userId":"17344282814232432823"}},"outputId":"1e8f3b7d-2ba6-426b-d9d8-d77a8834643f"},"source":["xv, yv = xy_from_file(('data20-1.csv'), 'Mother', 'Daughter')\n","find_line(xv, yv)\n","\n","#You should see y = 1.500x + 0.500. Which confirms the example."],"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["y = 1.500x + 0.500\n"]}]},{"cell_type":"markdown","metadata":{"id":"Mr0z1Nnqm--V"},"source":["#**Measuring the Strength of the the Linear Relationship**\n","\n","Given any dataset, we can always fit a line to it. And by definition it will be the best fitting line. But the question of how well the dependent variable's variation is explained by the dependent variable needs to be addressed.\n","\n","The following image depicts some of the important calculations we can make to help answer that question:\n","\n","![](https://drive.google.com/uc?export=view&id=1NpWZi0JWAma__5dl35s-swgXmYMmXiCL)"]},{"cell_type":"markdown","metadata":{"id":"oenCQjmvnKVT"},"source":["###**SST, SSE, MSE**\n","\n","SST is the total variation between the average y (dependent variable) and each point. It is the sum of the squared differences. Using NumPy it is simply:\n","\n","```\n","SST = np.sum(np.square(y - y_ave))\n","```\n","\n","SSE is the sum of the squared residuals. It is what the least squares method is minimizing. The **M**ean **S**quared **E**rrors is a scaled version of the SSE. It is the SSE divided by the number of observations. It will be a useful measurement in an upcoming lesson.\n","\n","###**R and R$^2$**\n","\n","As we saw, regression finds the best-fitting straight line; *correlation* is used to describe the strength of the relationship. The Pearson correlation coefficient, R, helps determine if two variables are strongly related. It basically measures how much of the variability of the dependent variable is explained by the independent variable. The Pearson correlation statistic measures this strength. It's range is -1 to +1. Values close to 0 imply the variables are unrelated.\n","\n","![](https://drive.google.com/uc?export=view&id=1kJTsM8rwjdmgjquT-847sG_JWEk0u4j2)\n","\n","$R^2$ is of course related to R but is easier to interpret. It is calculated via\n","\n","<div align=\"center\"> $R^2 = \\frac{Var(mean) - Var(line)}{Var(mean)}$ </div>\n","\n","\n","Var(mean)\n","* variance around the mean (looking at the y values)\n","* sum of the squared differences from the mean (between the actual y and the mean) \n","* SST (or SSM == SST/n)\n","\n","Var(line)\n","* variance around the actual and predicted values\n","* sum of the squared difference between actual value and predicted value \n","* SSE (or MSE == SSE/n)\n","\n","So based on our previous discussion, $R^2$ can also be defined as\n","\n","<div align=\"center\"> $R^2 = \\frac{SST - SSE}{SST}$ </div>\n","\n","\n","<br>\n","\n","####**R$^2$ as a Percentage**\n","\n","The $R^2$ range is [0, 1] and can be thought of as a percentage: it is the percentage of variation explained by the relationship between the two variables.\n","\n","####**20-1 example:**\n","\n","For the 20-1 data, $R^2$ is 0.94. The daughter/mother relationship accounts for 94% of the variation in the data.\n","\n","#**The p-value**\n","\n","![](https://drive.google.com/uc?export=view&id=1QasN_SXK4L84fbWXW51_XW4cCZQLenc6)\n","\n","From statistics, the p-value is the probability that random chance generated the data under investigation. It can be used to determine if $R^2$ is statistically significant or not. A low p-value, tells you if the $R^2$ value is due to chance (or something equal or rarer).\n","\n","Although we won't calculate it, several of the libraries we will use, do give you a p-value as well."]},{"cell_type":"markdown","metadata":{"id":"Y5r3B3HNpRZd"},"source":["\n","\n","#**Exercise**\n","#**(part of the Lesson requirements)**\n","**LinearRegressionOLS**\n","\n","Create a class named LinearRegressionOLS whose constructor takes an x and y array of values. It calculates (using only numpy) and saves the following as instance attributes (attribute name in parenthesis):\n","\n","* the x values (x)\n","* the y values (y)\n","* the slope (slope)\n","* the y intercept (y0)\n","* the SSE (SSE)\n","* r2, the correlation coefficient squared (r2)\n","\n","**Easy printing**\n","\n","Implement the method so that printing a LinearRegressionOLS instance will be similar to find_line.\n","\n","Most of the work has been done for you. It's now a matter of encapsulating all the information in a Python class."]},{"cell_type":"code","metadata":{"id":"0S4BsvJZp5WT","executionInfo":{"status":"ok","timestamp":1638301128957,"user_tz":360,"elapsed":119,"user":{"displayName":"Pratyush Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjdfJ0UzoDFRl5Z7hqJADmVKGbPH8sJ4KQ0IJsvg=s64","userId":"17344282814232432823"}}},"source":["class LinearRegressionOLS(object):\n","  def __init__(self, xv, yv):\n","    self.x = xv\n","    self.y = yv\n","    self.x_sum  = np.sum(self.x)\n","    self.y_sum  = np.sum(self.y)             \n","    self.xy_sum = np.sum(self.x * self.y)         \n","    self.x2_sum = np.sum(np.square(self.x))\n","    self.y2_sum = np.sum(np.square(self.y))  \n","    self.x_ave  = np.mean(self.x)\n","    self.y_ave  = np.mean(self.y)\n","    self.n = np.size(self.x)\n","    self.dx = (self.n * self.xy_sum - self.x_sum * self.y_sum)\n","    self.dy = (self.n * self.x2_sum - self.x_sum * self.x_sum)\n","    self.slope = self.dx/self.dy\n","    self.y0 = self.y_ave - self.slope * self.x_ave\n","    self.SSE = np.sum(np.square(self.y - (self.slope*self.x + self.y0)))\n","    self.SST = np.sum(np.square(self.y - self.y_ave))\n","\n","  def __repr__(self):\n","    return \"y = {:.3f}x + {:.3f}\".format(self.slope, self.y0)\n","\n","  def predict(self, x_value):\n","    return self.slope*x_value + self.y0\n","  def display(self, equal_aspect=False):\n","    fig, axes = plt.subplots(1,1)\n","    # plot the points\n","    axes.scatter(self.x, self.y, s=75, color='gray')\n","    # plot the fitted line\n","    x1 = np.min(self.x)\n","    y1 = self.predict(x1)\n","    x2 = np.max(self.x) + 0.25\n","    y2 = self.predict(x2)\n","    axes.plot([x1, x2], [y1,y2], color='blue')\n","    if equal_aspect:\n","      # make the axis' look nice\n","      max_v = np.max(np.maximum(self.x, self.y)) + 0.25\n","      axes.set_ylim(0, max_v)\n","      axes.set_xlim(0, max_v)\n","      axes.set_aspect('equal')\n","      return fig\n"],"execution_count":45,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VS2oZdRuqC2e"},"source":["Once done the following should work:\n","\n","```\n","# same data from Table 20-1\n","xv, yv = xy_from_file(('data20-1.csv'), 'Mother', 'Daughter')\n","stats_line = LinearRegressionOLS(xv, yv)\n","print(stats_line, \"({:.3f})\".format(stats_line.SSE))\n","\n","# data from Table 20-2; different dataset (same book, p.458)\n","xv, yv = xy_from_file(('data20-2.csv'), 'SUGAR', 'TIME')\n","stats_line = LinearRegressionOLS(xv, yv)\n","print(stats_line, \"({:.3f})\".format(stats_line.SSE))\n","```\n","\n","The output should be:\n","\n","```\n","y = 1.500x + 0.500 (1.000)\n","y = 2.906x + 7.312 (376.500)\n","```"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cdq36xuuQnB_","executionInfo":{"status":"ok","timestamp":1638301085822,"user_tz":360,"elapsed":125,"user":{"displayName":"Pratyush Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjdfJ0UzoDFRl5Z7hqJADmVKGbPH8sJ4KQ0IJsvg=s64","userId":"17344282814232432823"}},"outputId":"417c4264-f728-4a3c-ece5-36a04c2f08e9"},"source":["# same data from Table 20-1\n","xv, yv = xy_from_file(('data20-1.csv'), 'Mother', 'Daughter')\n","stats_line = LinearRegressionOLS(xv, yv)\n","print(stats_line, \"({:.3f})\".format(stats_line.SSE))\n","\n","# data from Table 20-2; different dataset (same book, p.458)\n","xv, yv = xy_from_file(('data20-2.csv'), 'SUGAR', 'TIME')\n","stats_line = LinearRegressionOLS(xv, yv)\n","print(stats_line, \"({:.3f})\".format(stats_line.SSE))"],"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["y = 1.500x + 0.500 (1.000)\n","y = 2.906x + 7.312 (376.500)\n"]}]},{"cell_type":"markdown","metadata":{"id":"oiefUuq6qT07"},"source":["### **Prediction**\n","\n","Of course we can use this model to make predictions as well -- that's one of the main reasons for using linear regression. Although the accuracy of the prediction is based on the quality of the data as well as the true correlation of the dependent variable (y) with the independent variable (x).\n","\n","Add the following method (predict) to your LinearRegressionOLS class so one can use it to make predictions. Once it is done the following should work:\n","\n","```\n","prediction = model.predict(x_value)\n","```\n","\n","###**Drawing the line**\n","\n","With the other methods in place, the next thing we want to do is draw the regression model. Add the following code to your class so we can display the data points and the fitted line:\n","\n","```\n","def display(self, equal_aspect=False):\n","\n","    fig, axes = plt.subplots(1,1)\n","\n","    # plot the points\n","    axes.scatter(self.x, self.y, s=75, color='gray')\n","\n","    # plot the fitted line\n","    x1 = np.min(self.x)\n","    y1 = self.predict(x1)\n","    x2 = np.max(self.x) + 0.25\n","    y2 = self.predict(x2)\n","    axes.plot([x1, x2], [y1,y2], color='blue')\n","\n","    if equal_aspect:\n","        # make the axis' look nice\n","        max_v = np.max(np.maximum(self.x, self.y)) + 0.25\n","        axes.set_ylim(0, max_v)\n","        axes.set_xlim(0, max_v)\n","        axes.set_aspect('equal')\n","  \n","  return fig\n","```\n","\n","Once this is done, you should have a nice looking graph that mimics the image taken from the textbook:\n","\n","![](https://drive.google.com/uc?export=view&id=1qI1oMiqImhwd-9cD1w8x3EwmM3_dkZ6y)"]},{"cell_type":"code","metadata":{"id":"0DBRAWTuqzqg","colab":{"base_uri":"https://localhost:8080/","height":302},"executionInfo":{"status":"ok","timestamp":1638301132972,"user_tz":360,"elapsed":272,"user":{"displayName":"Pratyush Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjdfJ0UzoDFRl5Z7hqJADmVKGbPH8sJ4KQ0IJsvg=s64","userId":"17344282814232432823"}},"outputId":"7a9d437e-14e2-4a15-87d0-7b564c67eb8a"},"source":["xv, yv = xy_from_file(('data20-1.csv'), 'Mother', 'Daughter')\n","stats_line = LinearRegressionOLS(xv, yv)\n","stats_line.display(True)\n","print(stats_line)"],"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["y = 1.500x + 0.500\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"u2NfONQWsnJY"},"source":["#**Finding the Line**\n","We now know that the goal of regression is to find the fitted line that minimizes the SSE (the objective function), but we haven't really explained how one goes from this formula:\n","\n","<div align=\"center\"> $SSE = \\sum_{i=1}^n (y_i - \\hat{y_i})^2$ </div>\n","\n","\n","to the calculations we used (from the textbook) to find the slope and y-intercept:\n","\n","![](https://drive.google.com/uc?export=view&id=14BE2p_bEaAAHTs6OP1RF8t7OUhHgNDnA)\n"]},{"cell_type":"markdown","metadata":{"id":"AC7eY-zdtFzK"},"source":["#**Enter stage right, calculus**\n","\n","![](https://drive.google.com/uc?export=view&id=1KJUaO0SM3BS2dajMnUYM8p-2tQhOsDJh)\n","\n","The answer to that important question lies on a path that goes through algebra and calculus.\n","\n","The idea is that you take the derivative of the objective function, set it to zero and go through algebraic hoops to arrive at the solution.\n","\n","The derivative of a function speaks to how the function changes with respect to a change in a variable or input. It measures the 'steepness' of the graph. Where the derivative is zero, is where the objective function (SSE) is at a minimum. The slope is zero -- it is not changing. Luckily we will not only skip the proof, but we will let you read it [here](https://storage.googleapis.com/uicourse/pdfs/RegressionProof.pdf) and [here](https://storage.googleapis.com/uicourse/dmap/SSE-derivations.pdf).\n","\n","This is the essence of differential calculus. If you have a function, there's a set of rules you can use to find that function's derivative. Once you have the derivative, you can set it to zero (where its slope is at a minimum) and solve for the other parts of the equation. So at the end of the derivation, the SSE function is at a minimum when the slope and y-intercept are set to the following values:\n","\n","* $y_0 = \\bar y - m \\bar x$\n","* $m = \\frac{S_{xy}}{S_{xx}}$\n","* $S_{xy} = \\sum(x_i - \\bar x)(y_i - \\bar y)$\n","* $S_{xx} = \\sum(x_i - \\bar x)^2$"]},{"cell_type":"markdown","metadata":{"id":"eUvLEL3rtl1S"},"source":["#**A small crack to let the ML light in.**\n","\n","The caveat to using calculus to minimize a cost function is that not all functions are differentiable. Also the above methods require you to accept using SSE as the cost function. This is where the machine learning people stepped in and said, Hey, we know how to deal with that situation(s). But we are getting ahead of ourselves.\n","\n","#**Welcome, Linear Algebra**\n","\n","###If you give a linear algebraist a set of equations, ...###\n","\n","The method we just outlined is the method of ordinary least squares (OLS) and why it's part of the class name you created. As you may have noticed, for a large dataset , there are many 'passes' over the data to build all those summations. When someone who lives and breaths linear algebra and sees things like $\\sum(xy)^2$, they immediately think of matrices to do that math.\n","\n","We can, through more mathematical gymnastics, solve for SSE using linear algebra. This is, we can find where SSE is at minimum using matrices.\n","\n","This is in fact, how most mathematical libraries implement regression. Perhaps you have heard of LAPACK and BLAS -- two very popular libraries for doing linear algebra (SciPy builds upon those libraries).\n","\n","**Closed Form Solutions**\n","\n","First we are going to present the 'closed form' solution. Then we will take it apart to understand how it works. It is considered a 'closed form' because there is an exact solution with a finite amount of data. To understand what is a closed form solution is, it might be best to look at something that is not:\n","\n"," <div align=\"center\"> $y = 4x + 6x^2 + \\frac{22}{3}x^3 + \\frac{95}{12}x^4 + ...$ </div>\n","\n","\n","That function is not in closed form because the summation never ends. However, the function $y = \\frac{2+x^2}{1-x}e^x$ does provide the same answer to the problem but with a closed form\n","solution."]},{"cell_type":"markdown","metadata":{"id":"ZDn3ZlHwuiyH"},"source":["#**Linear Algebra**\n","The closed form solution to find both the slope and y intercept expressed using matrices is \n","\n","<div align=\"center\"> $A = (X^TX)^{-1}X^TY$ </div>\n","\n","X, A, Y are defined as follows:\n","\n","<div align=\"center\"> $X = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ 1 & x_3 \\\\ .. & .. \\\\1&x_n\\end{bmatrix}$   $Y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ ..  \\\\y_n\\end{bmatrix}$  $A = \\begin{bmatrix} b \\\\ m \\end{bmatrix}$ </div>\n","\n","\n","The $^T$ is for *transform*, and the $^{−1}$ is for the matrix inverse (which we will go over).\n","\n","**Welcome Back •**\n","\n","Assuming we are a bit rusty on our linear algebra (and who isn't, really), let's go through it. The majority of operations are just two matrices multiplied together (i.e. the dot product). Below is a review of how dot product works with vectors and matrices:\n","\n","![](https://drive.google.com/uc?export=view&id=1G9QuxJPaDxCtOUaJpdQOtgd0II3jMs4C)\n","\n","\n","The dot product is perfect for multiplying and summing up quantities.\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"gLeCV_tEBG4g"},"source":["### **$(X^T X)$**\n","\n","The following shows the result of multiplying a matrix transform with itself:\n","\n","![](https://drive.google.com/uc?export=view&id=12E1i0uCxLlQsWu_EU12-fRRmafsINt42)\n","\n","The transform simply changes a column vector into a row vector (and vice versa). Note how the final answer contains some key summations (be sure to confirm the results with the OLS implementation)."]},{"cell_type":"code","metadata":{"id":"3B99iK6cN6Gc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638301521082,"user_tz":360,"elapsed":138,"user":{"displayName":"Pratyush Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjdfJ0UzoDFRl5Z7hqJADmVKGbPH8sJ4KQ0IJsvg=s64","userId":"17344282814232432823"}},"outputId":"03ab2927-b64d-45c1-acad-ae5eed00b271"},"source":["xv, yv = xy_from_file(('data20-1.csv'), 'Mother', 'Daughter')\n","\n","n = len(xv)\n","ones = np.ones(n)\n","X = np.column_stack( (ones, xv) )\n","print(X)\n","print(X.T)\n","XTX = X.T.dot(X)\n","print(XTX)"],"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1. 0.]\n"," [1. 0.]\n"," [1. 1.]\n"," [1. 2.]\n"," [1. 2.]\n"," [1. 3.]]\n","[[1. 1. 1. 1. 1. 1.]\n"," [0. 0. 1. 2. 2. 3.]]\n","[[ 6.  8.]\n"," [ 8. 18.]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"JV77CrLLN-gk"},"source":["### **$(X^T X)^{-1}$**\n","**Taking the inverse of a matrix $(^{−1})$**\n","\n","When you multiply a matrix by its inverse, you get the identity matrix (a matrix with all 0's except with 1's down the diagonal. A few algebraic steps were done to get to the following simplification of what $(X^T X)^{-1}$ becomes:\n","\n","![](https://drive.google.com/uc?export=view&id=1t7WX3MdhPd_iftdMwWpmuCaIYLUBZG-L)\n","\n","Also note that SSx is defined (as we saw earlier) as\n","\n","![](https://drive.google.com/uc?export=view&id=1qqIrG6m5RgURI4mPmTu8PSYjHmFBDL9_)"]},{"cell_type":"code","metadata":{"id":"AJt55wohOUCJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638306760932,"user_tz":360,"elapsed":127,"user":{"displayName":"Pratyush Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjdfJ0UzoDFRl5Z7hqJADmVKGbPH8sJ4KQ0IJsvg=s64","userId":"17344282814232432823"}},"outputId":"a0188bdd-7070-4a51-f44a-7272cf9a246e"},"source":["XTX_I = np.linalg.inv(XTX)\n","print(XTX_I)\n","\n","# show the identity matrix \n","print(XTX_I.dot(XTX))"],"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.40909091 -0.18181818]\n"," [-0.18181818  0.13636364]]\n","[[1.00000000e+00 6.10622664e-16]\n"," [0.00000000e+00 1.00000000e+00]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"mV67cTt7OU8N"},"source":["###**$(X^T X)^{-1}$**\n","\n","The final part gives us the ∑y and ∑(xy):\n","\n","![](https://drive.google.com/uc?export=view&id=1s6OCJ3dbgUoHkk1OLi5itEf4wTZvMEFw)"]},{"cell_type":"code","metadata":{"id":"LHF1VALFOo8v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638306767466,"user_tz":360,"elapsed":7,"user":{"displayName":"Pratyush Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjdfJ0UzoDFRl5Z7hqJADmVKGbPH8sJ4KQ0IJsvg=s64","userId":"17344282814232432823"}},"outputId":"a6c21144-043a-4695-8e14-3e72a767a5a6"},"source":["y = np.matrix(yv[:,]).T\n","print(y)\n","print(X.T.dot(y))"],"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0]\n"," [1]\n"," [2]\n"," [3]\n"," [4]\n"," [5]]\n","[[15.]\n"," [31.]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"e8vOm3snOyz_"},"source":["###**All together now: $(X^TX)^{−1}X^TY$**\n","\n","You can now put all the parts together and with using only linear algebra, you can solve for the best fitting line in one equation:"]},{"cell_type":"code","metadata":{"id":"msh3Fj7CO7Rl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638306776957,"user_tz":360,"elapsed":109,"user":{"displayName":"Pratyush Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjdfJ0UzoDFRl5Z7hqJADmVKGbPH8sJ4KQ0IJsvg=s64","userId":"17344282814232432823"}},"outputId":"dedbd39d-e1f8-49df-ff35-ab57a3a50a85"},"source":["A = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n","print(A)\n","y0    = A[0]\n","slope = A[1]\n","\n","stats_line = LinearRegressionOLS(xv, yv)\n","print(np.isclose(stats_line.slope,slope))\n","print(np.isclose(stats_line.y0,y0))"],"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.5]\n"," [1.5]]\n","[[ True]]\n","[[ True]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"2zUDeDBcO9aw"},"source":["Be mindful that it's essentially the same formulas used in the OLS solution.\n","\n","#**The DataScience Toolkit**\n","\n","NumPy, SciPy and scikit-learn (and many others) provide linear regression solvers. Depending on the selected algorithm, you can find both OLS and matrix implementations. All three provide access to their source code and you can [dig](https://github.com/scipy/scipy/blob/v1.5.4/scipy/stats/_stats_mstats_common.py#L15-L144) into the [details](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.lstsq.html#numpy.linalg.lstsq).\n","\n","**NumPy**\n","\n","Numpy's version is a bit tricky, since you need to get the data into matrix form:"]},{"cell_type":"code","metadata":{"id":"5GQ9jeyNPX29","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638306919529,"user_tz":360,"elapsed":119,"user":{"displayName":"Pratyush Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjdfJ0UzoDFRl5Z7hqJADmVKGbPH8sJ4KQ0IJsvg=s64","userId":"17344282814232432823"}},"outputId":"aa77fbd8-7bc7-4d8a-80dd-f700380173e9"},"source":["def numpy_lr(x,y):\n","  import numpy as np\n","  A = np.vstack([x, np.ones(len(x))]).T\n","\n","  # also scipy.linalg.lstsq\n","  p, res, rnk, s = np.linalg.lstsq(A, y, rcond=None)\n","  slope = p[0]\n","  y0    = p[1]\n","  \n","  result = \"y = {:.3f}x + {:.3f}\".format(slope, y0)\n","  print(result)\n","  \n","  r2 = 1 - res / np.sum((y - y.mean())**2)\n","  print('r^2', r2[0])\n","\n","xv, yv = xy_from_file(('data20-1.csv'), 'Mother', 'Daughter')\n","numpy_lr(xv,yv)"],"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["y = 1.500x + 0.500\n","r^2 0.942857142857143\n"]}]},{"cell_type":"markdown","metadata":{"id":"rJfE49UfPaup"},"source":["**SciPy**\n","\n","Scipy's version is the most straightforward (it also provides a p-value):"]},{"cell_type":"code","metadata":{"id":"pXrat2fQPgeY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638306947066,"user_tz":360,"elapsed":546,"user":{"displayName":"Pratyush Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjdfJ0UzoDFRl5Z7hqJADmVKGbPH8sJ4KQ0IJsvg=s64","userId":"17344282814232432823"}},"outputId":"96179b51-14fb-4708-8f0c-43444b4356e5"},"source":["def scipy_lr(x,y):\n","\n","  from scipy import stats\n","  \n","  slope, y0, r_value, p_value, std_err = stats.linregress(x, y)\n","  \n","  result = \"y = {:.3f}x + {:.3f}\".format(slope, y0)\n","  print(result)\n","  print('r^2', r_value**2)\n","\n","xv, yv = xy_from_file(('data20-1.csv'), 'Mother', 'Daughter')\n","scipy_lr(xv,yv)"],"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["y = 1.500x + 0.500\n","r^2 0.9428571428571431\n"]}]},{"cell_type":"markdown","metadata":{"id":"zyWoiki3PkKt"},"source":["**Sklearn**\n","\n","Sklearn's linear regression uses the familiar fit method."]},{"cell_type":"code","metadata":{"id":"qpZAfVlJPqXj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638307001049,"user_tz":360,"elapsed":394,"user":{"displayName":"Pratyush Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjdfJ0UzoDFRl5Z7hqJADmVKGbPH8sJ4KQ0IJsvg=s64","userId":"17344282814232432823"}},"outputId":"fbf6091d-2d2c-4feb-cdfe-879a74b6ba00"},"source":["def sklearn_lr(x,y):\n","\n","  from sklearn.linear_model import LinearRegression\n","  from sklearn.metrics import r2_score\n","  \n","  lr = LinearRegression().fit(x.reshape(-1, 1), y)\n","  slope = lr.coef_[0]\n","  y0    = lr.intercept_\n","  \n","  result = \"y = {:.3f}x + {:.3f}\".format(slope, y0)\n","  print(result)\n","  \n","  y_fit = xv * slope + y0\n","  print('r^2', r2_score(yv, y_fit))\n","\n","xv, yv = xy_from_file(('data20-1.csv'), 'Mother', 'Daughter')\n","sklearn_lr(xv,yv)"],"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["y = 1.500x + 0.500\n","r^2 0.9428571428571428\n"]}]},{"cell_type":"markdown","metadata":{"id":"9qWhiUexPtk5"},"source":["#**Extensions to linear regression**\n","**More Variables**\n","\n","Once you have a good handle on using linear regression to solve univariate problems, the extension (multi-variate regression) is simply adding more independent variables. The matrix math generalizes nicely.\n","\n","**Higher Order**\n","\n","You can also try to fit a curve to your dataset and move away from linear regression into polynomial regression. Usually one visualizes the data (or is getting poor results from linear regression) before deciding to fit a higher order polynomial to the dataset. You also risk overfitting the data as well when you add higher degree polynomials.\n","\n","![](https://drive.google.com/uc?export=view&id=1ucUzZo6FgTBEvu1f4bQbcVZvnPexaEu8)\n","\n","\n","\n","**Logistic Regression**\n","\n","Rather than predicting a continuous value, logistic regression is used when you want to predict if something is either True or False. It fits an 'S-curve' to the data rather than a straight line. We will see it at a later lesson.\n","\n","![](https://drive.google.com/uc?export=view&id=1Fqu_NmR_iKo5XNgLNcQgdDuNnUlKtr_m)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nlx-LddZQKpM"},"source":["#**Welcome Machine Learning**\n","\n","As you add more independent variables, both OLS and the matrix versions can spin out of control. OLS becomes a mess; the summations for working with just two independent variables already becomes a task of managing a lot of summations.\n","\n","The linear algebra solution does generalize nicely for adding additional attributes; however, finding the inverse of a matrix is computationally expensive, uses a lot of memory, and is only guaranteed to work under certain conditions.\n","\n","The fundamental approach to solving linear regression is to produce an answer using all the data -- all at once. There's another approach: iterative. That is, we solve linear regression by looking at each data point, one at a time. The process of doing this is called gradient descent.\n","\n","At its core, linear regression is parameter estimation. It attempts to estimate a slope and a y- intercept. However, the estimation, under certain conditions, is perfect. The method of gradient descent is ML's tour de force for parameter estimation.\n","\n","#**Gradient Descent (GD)**\n","\n","Gradient descent is NOT specific to linear regression, it can be used anytime you need to do parameter estimation. Gradient descent is a core algorithm used in many optimization problems found in statistics, machine learning and data science. It's also the main work- horse behind powering neural networks (updating the weights in back propagation).\n","\n","We will have a separate lesson on gradient descent (GD). But we will push the re- formulation of linear regression into the world of GD before we go.\n","\n","Let's start with the equation of a line:\n","\n","* $y = mx+b$\n","\n","Now change it into a function: \n","* $f(x) = mx + b$\n","\n","Replace the slope and y intercept with generic w$_i$eights \n","* $f(x) = w_0 + w_1x$\n","\n","Similarly, we can change the objective function MSE (SSE/n) from this \n","* $MSE = \\frac{1}{n}\\sum(y_i-(f(x_i)))^2$\n","\n","into this: \n","\n","* $MSE = \\frac{1}{2n}\\sum(y_i-(w_0 + w_1x_i))^2$\n","\n","\n","We use 1⁄2 of MSE -- to make the math easier (spoiler alert). This is essentially the cost function we will minimize in the next lesson.\n","\n","It is common convention to use a J (for Jacobian) to denote the cost function.\n","* $J(w_0,w_1) = \\frac{1}{2n}\\sum(y_i-(w_0 + w_1x_i))^2$ \n","\n","Some may use the word *loss* function to discuss a single training example and use *cost* function to measure with respect to the entire training set. The words however, are used (a lot) interchangeably.\n"]},{"cell_type":"markdown","metadata":{"id":"onjLKf_qRAaI"},"source":["#**Lesson Assignment**\n","#**Part 1: Finish LinearRegressionOLS**\n","Make sure your LinearRegressionOLS class is working as outlined. It will be tested against the Table 20-2 dataset which measures sugar consumption (independent variable) and activity scores (dependent variable).\n","#**Part 2: Gradient Descent Preparation**\n","You will create a class named LinearRegressionGD whose constructor takes an x and y numpy array (named xv and yv).\n","\n","You will also create a method named plot_mse that plots the MSE (mean squared error) for various values for either the slope or y-intercept of a line. The method signature is discussed below.\n","\n","For example the graph below shows various values used for y0 and the resulting MSE value (for the 20-2 dataset):\n","\n","![](https://drive.google.com/uc?export=view&id=1-NtW4kZ2IeLjvyz_M5ndAGraRKJs_Qmz)\n","\n","Each point on the graph is the result of creating a line with the given y-intercept and then calculating the MSE for that particular line.\n","\n","For this lesson, use the following definition of MSE:\n","\n","<div align = \"center\"> $MSE = \\frac{1}{n}\\sum(y_i-(w_0 + w_1x_i))^2$ </div>\n","\n","\n","\n","\n","**plot_mse**\n","\n","Create a method named plot_mse which will plot MSE on the y-axis and either slope or the y-intercept on x-axis.\n","* plot_mse has 3 parameters: v_values, slope=None, y0=None\n","* v_values is the vector of either slopes or y0's to plot\n","* it creates a figure (i.e. fig, axes = plt.subplots(figsize=(6,6)) for plotting \n","* return the figure"]},{"cell_type":"code","metadata":{"id":"wcBu8epqQJn-","executionInfo":{"status":"ok","timestamp":1638313537355,"user_tz":360,"elapsed":118,"user":{"displayName":"Pratyush Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjdfJ0UzoDFRl5Z7hqJADmVKGbPH8sJ4KQ0IJsvg=s64","userId":"17344282814232432823"}}},"source":["import matplotlib.pyplot as plt\n","\n","class LinearRegressionGD(object):\n","  def __init__(self, xv, yv):\n","    self.x = xv\n","    self.y = yv\n","    self.n = np.size(xv)\n","\n","  def plot_mse(self, v_values, slope = None, y0 = None):\n","    MSE_list = []\n","    fig, axes = plt.subplots(figsize=(6,6))\n","    if (slope is None and y0 is not None):\n","      plot_slope = v_values\n","      for i in plot_slope:\n","        value = np.sum(np.square(self.y - (i*self.x + y0)))/self.n\n","        MSE_list.append(value)\n","        axes.set_xlabel('slope')\n","    elif (slope is not None and y0 is None):\n","      plot_y0 = v_values\n","      for i in plot_y0:\n","        value = np.sum(np.square(self.y - (slope*self.x + i)))/self.n\n","        MSE_list.append(value)\n","        axes.set_xlabel('y0')\n","    MSE = np.array(MSE_list)\n","    # plot the points\n","    axes.scatter(v_values, MSE, s=75, color='blue')\n","    axes.set_ylabel('MSE')\n","    return fig\n","    \n"],"execution_count":94,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FJLNE_3nRjpS"},"source":["Be sure to label your graphs properly. Use the variables plot_y0 and plot_slope to help navigate the input vector (v_values).\n","\n","```\n","plot_y0    = y0 is None and slope is not None\n","plot_slope = slope is None and y0 is not None\n","assert plot_y0 or plot_slope, 'bad call'\n","```"]},{"cell_type":"markdown","metadata":{"id":"fd_U4B6mRqA5"},"source":["#**Testing**\n","Be sure to test your function locally using the two provided datasets. For example, if you did the following:"]},{"cell_type":"code","metadata":{"id":"44C6mU7WRvsB","colab":{"base_uri":"https://localhost:8080/","height":779},"executionInfo":{"status":"ok","timestamp":1638313552108,"user_tz":360,"elapsed":457,"user":{"displayName":"Pratyush Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjdfJ0UzoDFRl5Z7hqJADmVKGbPH8sJ4KQ0IJsvg=s64","userId":"17344282814232432823"}},"outputId":"094c60cc-e3a5-4f65-b6da-52d61c28447d"},"source":["# load up the dataset\n","xv, yv = xy_from_file(('data20-2.csv'), 'SUGAR', 'TIME')\n","lr_gd = LinearRegressionGD(xv,yv)\n","\n","# from our linear regression analysis, we know\n","# y = 2.90625x + 7.31250\n","\n","# generate different values for y0\n","y0_v = np.linspace(-10,20,50)\n","# add in the known 'best' y0\n","y0_v = np.sort(np.append(y0_v, 7.31250))\n","\n","# plot different y0's against MSE\n","# note the named parameter slope (whose default value is None)\n","lr_gd.plot_mse(y0_v, slope=2.90625)"],"execution_count":95,"outputs":[{"output_type":"execute_result","data":{"image/png":"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\n","text/plain":["<Figure size 432x432 with 1 Axes>"]},"metadata":{},"execution_count":95},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x432 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"ykQZ6zWPRyXm"},"source":["You would get the following graph:\n","\n","![](https://drive.google.com/uc?export=view&id=1oEz9YA45A2yhZvcNrepvtVJ8D2Z3jZHY)"]},{"cell_type":"markdown","metadata":{"id":"UNlrNop4R19Q"},"source":["Don't confuse quadratic as degree 4, In Latin, the word \"quadrus\" means a square (because it has four sides) and \"quadratus\" means \"squared.\"\n","\n","![](https://drive.google.com/uc?export=view&id=1VsEvWdrL_qs191RfnrBt3J1tduUkgv7f)\n","\n","\n","Quadratus is the Latin for \"square\" due to there being four sides on a square. The second power of a number is called its square because if we have an integer, and construct a square with that number of items on each side, the total number will be its second power. For example, a 4×4 square having 16 items:\n","```\n","* * * *\n","* * * *\n","* * * *\n","* * * *\n","```\n","Sources http://mathforum.org/library/drmath/view/52572.html"]},{"cell_type":"markdown","metadata":{"id":"M597RpnkEANF"},"source":["**Steps to submit your work:**\n","\n","\n","1.   Download the lesson notebook from Moodle.\n","2.   Upload any supporting files using file upload option within Google Colab.\n","3.   Complete the exercises and/or assignments\n","4.   Download as .ipynb\n","5.   Name the file as \"lastname_firstname_WeekNumber.ipynb\"\n","6.   After following the above steps, submit the final file in Moodle\n","\n","\n","\n","\n","\n","<h1><center>The End!</center></h1>"]}]}