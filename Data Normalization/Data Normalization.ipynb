{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week8_Data_Normalization_Part1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CjKbICfT7OOM"},"source":["#**Data Normalization (part 1)**\n","\n","#**Normalizing & Scaling data**\n","\n","This lesson is about preparing your data so that machine learning algorithms can be more accurate and efficient. We will discuss various techniques that all fall under the categories of data standardization, cleaning, scaling, etc. We'll use the umbrella term normalization. Although even that term means something different when talking about vector normalization.\n","\n","The overall goal of data normalization is about reducing redundancy and improving accuracy and integrity of the data and for any techniques that will use the data. The process of normalizing has its roots in relational databases where the goal is to restructure the tables and relationships to get them into 'normal' form to reduce data redundancy."]},{"cell_type":"markdown","metadata":{"id":"jwWJ7-OK7oAA"},"source":["#**Different Data Types, different techniques**\n","\n","\n","![](https://drive.google.com/uc?export=view&id=1UvAkd0P4H_lLomxN3qQwsHOpE5VzcWRV)\n","\n","You learned about the different ways to classify data in the previous class; this lesson is looking at data at more granular level. Although all data is essentially grouped binary values (bits on or off), we will look at how to manage some of the most common categories in data- science including text, numeric, and categorical (a mix of text and numbers).\n","\n","Data normalization can be done just about any kind of data including audio, image, text, numeric, and categorical data. We will go over some of the basic strategies for handling several of these data types.\n","\n","We will have an additional lesson that focuses on processing text data. What follows are some common techniques we can use to apply rules to help bring some consistency to handling both numeric and categorical attributes.\n"]},{"cell_type":"markdown","metadata":{"id":"xxcUG0e3-nmE"},"source":["#**Cleaning the Missing**\n","\n","Many of the techniques to 'normalize' the data will fail if the transformation is done on a missing value. Missing values can truly be absent (e.g. the dreaded double comma in csv files) or marked with a special character like None, NAN, NaN, nan,null, NULL, void, 'n/a', or the empty string (i.e ''). Pandas, NumPy, and Sklearn all provide ways to help with cleaning and normalizing data."]},{"cell_type":"markdown","metadata":{"id":"nsb0QfLKCLoW"},"source":["#**A titanic example**\n","\n","![](https://drive.google.com/uc?export=view&id=1T62pGhC6VkO8Ya1jJe0hvZUvooEOjtDL)\n","\n","The Titanic sank on April 15th, 1912 after hitting an iceberg. It had roughly 2208 passengers and crew aboard (the exact number seems to be [unknown](http://www.icyousee.org/titanic.html).) A few good [references](https://www.historyonthenet.com/the-titanic) for some [details](https://titanicfacts.net/titanic-passengers/) show the discrepancies in the exact numbers.\n","\n","The titanic dataset provides a good opportunity to work on a classic dataset that is in need of some cleaning and normalization. This lesson includes\n","one of the more complete datasets. There are many available.\n","\n","We can load up the dataset and print out the first row so you can get a feel for the data. You can [read](http://campus.lakeforest.edu/frank/FILES/MLFfiles/Bio150/Titanic/TitanicMETA.pdf) about the meaning of each data field as well."]},{"cell_type":"code","metadata":{"id":"OpV0uMNrC4XV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634691162309,"user_tz":300,"elapsed":321,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}},"outputId":"ea87d738-9d83-450a-c56c-7ec2107c6a42"},"source":["import pandas as pd\n","import numpy as np\n","\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.width', 200)\n","\n","def build_titanic():\n","  df = pd.read_csv('titanic.csv')\n","  print('total rows', len(df))\n","\n","  # add an extra passenger\n","  extra = {'name': 'Jack Dawson', 'age': 28, 'id': len(df)+1, 'gender': 'male'}\n","  df = df.append(extra, ignore_index=True)\n","\n","  # add an extra field for using a custom transformer\n","  df['sid'] = df['age'].apply(lambda x: 'NA' if np.isnan(x) else \"{:.0f}\".format(1912-\n","x)).astype('string')\n","  df['sid'].replace('NA', np.nan, inplace=True)\n","  \n","  return df.copy()\n","\n","df = build_titanic()\n","print(df.head(1))"],"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["total rows 2207\n","   id                 name gender   age class embarked        country  ticketno  fare  sibsp  parch survived   sid\n","0   1  Abbing, Mr. Anthony   male  42.0   3rd        S  United States    5547.0  7.11    0.0    0.0       no  1870\n"]}]},{"cell_type":"markdown","metadata":{"id":"rlti5VSAC7kH"},"source":["**Getting the missing counts**\n","\n","One of the first things you should do is get a count of which attributes have missing values. You can easily do this with pandas. In the example below, we ask for all null (NaN) values and then sum them up. When you look at the result, any column with a non-zero value has missing data."]},{"cell_type":"code","metadata":{"id":"SKUEDfihDEKy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634691165540,"user_tz":300,"elapsed":560,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}},"outputId":"dcd34906-f5f5-4215-e3ac-39fa90b2335b"},"source":["def show_missing(df):\n","  print(df.isna().sum())\n","  \n","show_missing(df)"],"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["id            0\n","name          0\n","gender        0\n","age           2\n","class         1\n","embarked      1\n","country      82\n","ticketno    892\n","fare        917\n","sibsp       901\n","parch       901\n","survived      1\n","sid           2\n","dtype: int64\n"]}]},{"cell_type":"markdown","metadata":{"id":"zvsRJK4-DFC6"},"source":["You can see that 'age' has only 2 missing values. That is, 2 rows/instances don't have a value for the 'age' column. If a majority of your rows have missing values for a certain attribute, the best strategy is to simply not use that attribute in any of the analysis (e.g. country, ticketno, fare, sibsp, parch)."]},{"cell_type":"markdown","metadata":{"id":"Z2SLbfTfDMtl"},"source":["### **Imputing the missing**\n","\n","One of the most common ways to deal with missing values is to interpolate (e.g. estimate or impute) the value from the values that are present.\n","\n","**Pandas**\n","\n","In the example below, we fill empty/missing values for the 'age' attribute with the mean for that column. You can also use the calculated median, mode or a constant as well."]},{"cell_type":"code","metadata":{"id":"ElM-UWZiDZ2v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634691169166,"user_tz":300,"elapsed":383,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}},"outputId":"4c23648a-c32c-49b2-df9b-124128876139"},"source":["def process_missing_age(df, debug=True):\n","\n","  # mask to select the rows where age is empty\n","  mask = df.age.isna()\n","\n","  # calculate the mean (the replacement value)\n","  replace_value = df.age.mean()\n","  \n","  # fill those values with the value calculated\n","  df['age_clean']= df[mask].age.fillna(replace_value)\n","  if debug:\n","    # print out the updates\n","    cols = ['id', 'name', 'age', 'age_clean']\n","    print(df[mask][cols].head())\n","    \n","# pass in a copy, so we keep the original\n","process_missing_age(df.copy())"],"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["      id                    name  age  age_clean\n","439  440  Gheorgheff, Mr. Stanio  NaN   30.43563\n","677  678     Kraeff, Mr. Theodor  NaN   30.43563\n"]}]},{"cell_type":"markdown","metadata":{"id":"cADYG_kpDeG9"},"source":["**Sklearn**\n","\n","Another way to impute the missing is to use sklearn. The same 'age' column is updated to the mean for any missing value. Note how we create a new attribute to hold the 'age' column that is free of any missing values. \n","\n","Also, read the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) for different strategies you can use. Note sklearn's repeated fit and transform process is applied here (even though there is no training and fitting of models)."]},{"cell_type":"code","metadata":{"id":"DQJrCi1yDdPx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634691172132,"user_tz":300,"elapsed":238,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}},"outputId":"43d5bf18-2c9e-4569-feae-042417121caa"},"source":["def process_missing_age_skl(df, debug=True):\n","  from sklearn.impute import SimpleImputer\n","  import numpy as np\n","\n","  # np.nan is the how pandas marks missing values\n","  # replace with the mean\n","  imr = SimpleImputer(strategy=\"mean\", missing_values=np.nan)\n","\n","  # Impute values\n","  values = df.age.values.reshape(-1,1)\n","  out = imr.fit_transform(values)\n","\n","  # now we assign those values to a new column\n","  df['im_age'] = out\n","\n","  if debug:\n","    cols = ['id', 'name', 'age', 'im_age']\n","    mask = df.age.isna()\n","    print(df[mask][cols].head())\n","    print('Missing')\n","    print(df.isna()[cols].sum())\n","    \n","process_missing_age_skl(df.copy())"],"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["      id                    name  age    im_age\n","439  440  Gheorgheff, Mr. Stanio  NaN  30.43563\n","677  678     Kraeff, Mr. Theodor  NaN  30.43563\n","Missing\n","id        0\n","name      0\n","age       2\n","im_age    0\n","dtype: int64\n"]}]},{"cell_type":"markdown","metadata":{"id":"jvflS3VbDrvH"},"source":["A few lines of code to note. See if you understand the following from the above code block:\n","\n","\n","\n","```\n","# Impute values\n","values = df.age.values.reshape(-1,1)\n","out = imr.fit_transform(values)\n","```\n","\n","* sklearn needs its data in numpy/array form\n","* .values is the underlying numpy array\n","* .reshape(-1,1) reshapes the data to a single feature/column\n","* -1,1 means all rows, 1 column\n","\n","It's also possible to use a more 'user-friendly' version as well:\n","\n","```\n","# Impute values\n","features = ['age']\n","values = df[features]\n","out = imr.fit_transform(values)\n","```\n","\n","In this case, we can actually fit and transform multiple columns. If you do that it's important to take caution on how you assign the result back to the pandas dataframe."]},{"cell_type":"markdown","metadata":{"id":"I_7rqMW0FUw9"},"source":["**Deleting the missing instances (rows)**\n","\n","Another, perhaps drastic, technique is to simply remove any row that has a missing value for any of the attributes you need. This strategy is fine if you have plenty of data. You can also decide to delete only if an instance has multiple missing attributes.\n","\n","**Predicting the missing;**\n","\n","Another viable option is to use a machine learning algorithm to figure out which value should be used to replace the missing. Although we haven't discussed the K-Nearest Neighbors (KNN) algorithm (a supervised ML algorithm for both classification and regression), it's a viable solution if you want a more robust strategy than simple statistical (e.g. mean, median, mode) replacement. If there's enough time, we will have a lesson on KNN."]},{"cell_type":"markdown","metadata":{"id":"fFM-Dv75FdFR"},"source":["#**Categorical Data**\n","\n","![](https://drive.google.com/uc?export=view&id=1_8d2JROR2Oa4_XMi2UIQEkdqi4YhQJnV)\n","\n","For handling categorical data or data whose attribute values are labels/text, the main focus is being consistent for handling the different categories. The goal is to map each label/category to a unique number.\n","\n","Many of the same rules apply for text cleaning; however, if the label is coming from a computer, process, sensor, or standardized input, much of the cleaning is already done for you. That is, you don't have to worry about different labels having the same semantic meaning. For example, if your categories are coming from a web form, you would have to deal with misspellings, etc."]},{"cell_type":"markdown","metadata":{"id":"6qDsOxroF1nK"},"source":["###**Mapping labels to numbers**\n","\n","All machine learning algorithms need a numeric representation of any text value. If you have categorical attributes whose values are strings, you will need to map these values to a number.\n","\n","**Nominal Attributes**\n","For simple categorical data where there is no ranking order of the values, we can simply map the labels to numbers. In Pandas, it's very straight forward. You can use the .astype(\"category\").cat.codes on any categorical data type:\n","\n","```\n","df['g_code'] = df['gender'].astype(\"category\").cat.codes\n","```\n","Because the process is so straight forward, we can create a utility function to help us map any column:"]},{"cell_type":"code","metadata":{"id":"ddzAN72OFSLh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634691179916,"user_tz":300,"elapsed":342,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}},"outputId":"5fb31069-9ec6-45d6-a687-e660cf733f01"},"source":["def category_to_number(df, col_name, new_name, debug=True):\n","\n","  # map the categories to unique integers (starting at 0)\n","  df[new_name] = df[col_name].astype(\"category\").cat.codes\n","\n","  if debug:\n","    values = df[col_name].unique()\n","    print('{:d} unique values for {:s}:'.format(len(values), col_name), values)\n","    # show how many rows are in each group\n","    print(df.groupby([col_name]).count()[new_name].reset_index())\n","    print(df.groupby([col_name]).count()[new_name].sum())\n","  \n","  return df\n","\n","# map gender to a 0/1 code\n","cols = ['gender', 'g_code']\n","df2 = category_to_number(df.copy(), *cols)  # putting * to good use\n","print(df2[cols][0:10]) # first 10 rows"],"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["2 unique values for gender: ['male' 'female']\n","   gender  g_code\n","0  female     489\n","1    male    1719\n","2208\n","   gender  g_code\n","0    male       1\n","1    male       1\n","2    male       1\n","3  female       0\n","4  female       0\n","5    male       1\n","6    male       1\n","7  female       0\n","8    male       1\n","9    male       1\n"]}]},{"cell_type":"markdown","metadata":{"id":"hbCJiwDYGSBO"},"source":["**Handling missing values**\n","\n","For the 'gender' column, there are no missing values. However if there are any missing values, those values get marked with a -1 by default. If you want to keep the np.nan values for missing you can simply do a replace:\n","\n","```\n","df2['g_code'] = df2['g_code'].replace(-1, np.nan)\n","```\n","\n","**port of embarkation**\n","We can use the same function to map port of embarkation, which does have some missing values. Let's take a look at how pandas handles those missing values. Be sure to understand how pandas treats the missing values"]},{"cell_type":"code","metadata":{"id":"7h0cyTfuGmQd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634691186940,"user_tz":300,"elapsed":260,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}},"outputId":"ead121e5-af4e-4443-8128-adc065e67002"},"source":["# map port of embarkation (C, Q, S)\n","# C = Cherbourg, Q = Queenstown, S = Southampton,  B = ??\n","cols = ['embarked', 'e_code']\n","df = category_to_number(df.copy(), *cols)\n","print(df[cols][0:10]) # first 10 rows\n","mask = df.embarked.isna()\n","print(df[mask])"],"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["5 unique values for embarked: ['S' 'C' 'B' 'Q' nan]\n","  embarked  e_code\n","0        B     197\n","1        C     271\n","2        Q     123\n","3        S    1616\n","2207\n","  embarked  e_code\n","0        S       3\n","1        S       3\n","2        S       3\n","3        S       3\n","4        S       3\n","5        S       3\n","6        C       1\n","7        C       1\n","8        C       1\n","9        S       3\n","        id         name gender   age class embarked country  ticketno  fare  sibsp  parch survived   sid  e_code\n","2207  2208  Jack Dawson   male  28.0   NaN      NaN     NaN       NaN   NaN    NaN    NaN      NaN  1884      -1\n"]}]},{"cell_type":"markdown","metadata":{"id":"_X-gSSipGprX"},"source":["**Codebook WARNING**\n","\n","As you may have noticed, there's not much control over which numbers will be assigned to which categories. If you needed 'Cherbourg' to be a specific numeric value (because of an outside requirement or codebook -- a document that specifies how the mappings are done), you would have to adjust the algorithm. The next section discusses, one possible solution."]},{"cell_type":"markdown","metadata":{"id":"W_sdFW9MGzms"},"source":["**Ordinal Mapping**\n","When your categorical attributes have an inherent ranking (e.g. review of stars, likert scales, etc), you can define your own value map that maintains the order as well."]},{"cell_type":"markdown","metadata":{"id":"jik_xaUPHFoc"},"source":["**“ Likert's Log**: As a word of caution, just because there's an order of 'rank' to your values, these values are still not 'numeric'.\n","\n","For example, if you had a survey that ranked items using a [Strongly Agree, Agree, Neither, Disagree, Strongly Disagree] scale or you asked someone to rank an issue from 1 to 5, don't assume you can work with averages. Also, the interval between different values isn't mathematically stable. For example, the difference between 'Strongly Agree' and 'Agree' cannot be assumed to be the same as the difference between 'Disagree' and 'Strongly Disagree'. Using counts, medians, and modes is usually the best you can do.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QDBGhVX1HTGF"},"source":["With Pandas, you can use the map function to create custom rank values:\n","\n","```\n","df[new_name] = df[column_name].map(kv_map)\n","```\n","\n","Let's use that pattern to map the class attribute that categorizes both the passengers and crew. Note that all the crew gets the value 4."]},{"cell_type":"code","metadata":{"id":"9M0GqHzSGpMH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634691190560,"user_tz":300,"elapsed":396,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}},"outputId":"ff5714d3-dcd4-4254-dcbd-7991e4e4a527"},"source":["def map_class_attribute(df, debug=True):\n","\n","  # map 1st/2nd/3rd class as ordinal 1st < 2nd < 3rd + missing\n","  # replace nan with the value 'unknown'\n","  df['class'].fillna('unknown', inplace=True)\n","  # assign the labels values\n","  ord_map = {'3rd':3, '2nd':2, '1st':1,\n","             'engineering crew':4,\n","             'victualling crew':4,\n","             'restaurant staff':4,\n","             'deck crew':4, 'unknown':0}\n","\n","  # apply the map to the 'class' attribute\n","  df['o_class'] = df['class'].map(ord_map)\n","\n","  if debug:\n","    print(df['class'].unique().tolist())\n","    # show how many rows are in each group\n","    print(df.groupby(['class']).count()['o_class'].reset_index())\n","\n","  return df\n","  \n","df_class = map_class_attribute(df.copy())\n","print(df_class.head())"],"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["['3rd', '2nd', '1st', 'engineering crew', 'victualling crew', 'restaurant staff', 'deck crew', 'unknown']\n","              class  o_class\n","0               1st      324\n","1               2nd      284\n","2               3rd      709\n","3         deck crew       66\n","4  engineering crew      324\n","5  restaurant staff       69\n","6           unknown        1\n","7  victualling crew      431\n","   id                            name  gender   age class embarked        country  ticketno   fare  sibsp  parch survived   sid  e_code  o_class\n","0   1             Abbing, Mr. Anthony    male  42.0   3rd        S  United States    5547.0   7.11    0.0    0.0       no  1870       3        3\n","1   2       Abbott, Mr. Eugene Joseph    male  13.0   3rd        S  United States    2673.0  20.05    0.0    2.0       no  1899       3        3\n","2   3     Abbott, Mr. Rossmore Edward    male  16.0   3rd        S  United States    2673.0  20.05    1.0    1.0       no  1896       3        3\n","3   4  Abbott, Mrs. Rhoda Mary 'Rosa'  female  39.0   3rd        S        England    2673.0  20.05    1.0    1.0      yes  1873       3        3\n","4   5     Abelseth, Miss. Karen Marie  female  16.0   3rd        S         Norway  348125.0   7.13    0.0    0.0      yes  1896       3        3\n"]}]},{"cell_type":"markdown","metadata":{"id":"fHSF1tGDIAZz"},"source":["Once we have those numeric values, it's pretty easy to separate everyone into crew and passengers:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i1oYMMgGIDN_","executionInfo":{"status":"ok","timestamp":1634691194043,"user_tz":300,"elapsed":401,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}},"outputId":"6ec72cde-f617-4621-e205-16a9b361f5cb"},"source":["def print_passenger_class_stats(df):\n","  is_crew = df['o_class'].isin([4])\n","\n","  # any of these will work\n","  is_pass = df['o_class'].isin([1,2,3])\n","  is_pass = (df['o_class'] < 4 ) & (df['o_class'] > 0)\n","  is_pass = ~is_crew  # will include the unknowns\n","\n","  print('crew', len(df[is_crew]))\n","  print('pass', len(df[is_pass]))\n","  \n","# this assumes map_class_attribute is done\n","print_passenger_class_stats(df_class.copy())"],"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["crew 890\n","pass 1318\n"]}]},{"cell_type":"markdown","metadata":{"id":"5OKqp-PEIGYA"},"source":["**Binary Fields**\n","\n","Sometimes you may want an attribute to simply indicate a simple Yes/No, On/Off, Have/not- Have value.\n","\n","For example, we can add a is_passenger attribute to the data. This field will either be 0 (False) or 1 (True) indicating if the person was a passenger on the ship (as opposed to being part of the crew.)"]},{"cell_type":"code","metadata":{"id":"peUkytXPIPxk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634691196748,"user_tz":300,"elapsed":285,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}},"outputId":"8d6bec58-71ef-47bf-f6c7-9f36eeaf5c6a"},"source":["def create_binary_field(df, debug=True):\n","  # Binary Fields\n","  from sklearn.preprocessing import Binarizer\n","  binarizer = Binarizer(threshold=3, copy=True)  # <= 3 asssing\n","  column_values = binarizer.fit_transform(df.o_class.values.reshape(-1, 1))\n","  \n","  # flip the values (1 -> 0; 0 -> 1)\n","  df['is_passenger'] = 1 - column_values\n","  if debug:\n","    print(column_values)\n","    print(df.head(5))\n","\n","print(create_binary_field(df_class.copy()))"],"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0]\n"," [0]\n"," [0]\n"," ...\n"," [1]\n"," [1]\n"," [0]]\n","   id                            name  gender   age class embarked        country  ticketno   fare  sibsp  parch survived   sid  e_code  o_class  is_passenger\n","0   1             Abbing, Mr. Anthony    male  42.0   3rd        S  United States    5547.0   7.11    0.0    0.0       no  1870       3        3             1\n","1   2       Abbott, Mr. Eugene Joseph    male  13.0   3rd        S  United States    2673.0  20.05    0.0    2.0       no  1899       3        3             1\n","2   3     Abbott, Mr. Rossmore Edward    male  16.0   3rd        S  United States    2673.0  20.05    1.0    1.0       no  1896       3        3             1\n","3   4  Abbott, Mrs. Rhoda Mary 'Rosa'  female  39.0   3rd        S        England    2673.0  20.05    1.0    1.0      yes  1873       3        3             1\n","4   5     Abelseth, Miss. Karen Marie  female  16.0   3rd        S         Norway  348125.0   7.13    0.0    0.0      yes  1896       3        3             1\n","None\n"]}]},{"cell_type":"markdown","metadata":{"id":"zRJbYCpfITOk"},"source":["**One Hot Encoding**\n","\n","As mentioned in a previous lesson, one hot encoding simply assigns either a 1 or a 0 to an attribute if that attribute is present or not. It's an extension of using the Binarizer. The issue is that for categorical (nominal or ordinal), it's not accurate that some values will be 'higher' than other only because their mapping value was larger. As we just saw, if you map category labels, for example, to the numbers 0 through 100 (or 0.0 to 1.0), the ML algorithm may think the higher values contributes more to some correlation or calculation than the lower values. It's also useful just to mark some attributes as being present or not. This is where one hot encoding becomes useful.\n","<br>\n","<br>\n","\n","In the previous section we built a simple one-hot attribute (is_passenger), this example takes an entire column of categorical values and builds separate one-hot columns for each unique value. It's simply a convenient way to create one-hot attributes. The image below shows an example of the result of one hot encoding of the embankment attribute:\n","\n","\n","![](https://drive.google.com/uc?export=view&id=1tBf_U9C70qAmBFZgcqPltDDbv5ueC1DZ)"]},{"cell_type":"code","metadata":{"id":"EIrajGiuIeVY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634691201322,"user_tz":300,"elapsed":484,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}},"outputId":"3019b44b-3255-4451-8ead-168524d41a92"},"source":["def one_hot_encoding(df):\n","  import numpy as np\n","  from sklearn.preprocessing import OneHotEncoder\n","\n","  onehot = OneHotEncoder(dtype=np.int, sparse=True)\n","  \n","  # fill in any missing values with 'UNK'\n","  df['embarked'].fillna('UKN', inplace=True)\n","  values = df['embarked'].values.reshape(-1, 1)\n","  values = onehot.fit_transform(values).toarray() # it is sparse\n","  labels = onehot.categories_\n","  \n","  return pd.DataFrame(values, columns=labels)\n","\n","df_hot = one_hot_encoding(df.copy())\n","print(df_hot.tail(20))"],"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["      B  C  Q  S UKN\n","2188  0  0  0  1   0\n","2189  0  0  0  1   0\n","2190  0  0  0  1   0\n","2191  0  0  0  1   0\n","2192  0  0  0  1   0\n","2193  0  0  0  1   0\n","2194  0  0  0  1   0\n","2195  0  0  0  1   0\n","2196  0  0  0  1   0\n","2197  0  0  0  1   0\n","2198  1  0  0  0   0\n","2199  0  0  0  1   0\n","2200  0  0  0  1   0\n","2201  0  0  0  1   0\n","2202  1  0  0  0   0\n","2203  0  0  0  1   0\n","2204  0  0  0  1   0\n","2205  0  0  0  1   0\n","2206  0  0  0  1   0\n","2207  0  0  0  0   1\n"]}]},{"cell_type":"markdown","metadata":{"id":"u9CetVdXIgxR"},"source":["The panda's get_dummies method provides another way to do one-hot encoding:\n","\n","```\n","print(pd.get_dummies(df['embarked'], dummy_na=True))\n","```"]},{"cell_type":"markdown","metadata":{"id":"rNy2W4tiIlnk"},"source":["#**Binning/Discretizating Features**\n","Although not strictly for categorical data, another option is to put data into bins. Pandas provides the cut method to create custom bins:"]},{"cell_type":"code","metadata":{"id":"fNQRcpw1IsjR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634691205623,"user_tz":300,"elapsed":282,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}},"outputId":"f13fd1c1-968a-4e5a-9692-8aeef0806c4d"},"source":["# binning data\n","def bin_demo1(df):\n","# set up custom bins\n","  bins = [0, 3, 8, 16, 21, 35, 55, 200] \n","  labels = ['infant','child','youth','young adult','adult','middle','senior'] \n","  age_bins = pd.cut(df['age'], bins=bins, labels=labels, right=False) \n","  df['age_cat'] = age_bins\n","  print(df['id age age_cat'.split()].head(10))\n","  return df\n","  \n","df_bin = bin_demo1(df.copy())"],"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["   id   age      age_cat\n","0   1  42.0       middle\n","1   2  13.0        youth\n","2   3  16.0  young adult\n","3   4  39.0       middle\n","4   5  16.0  young adult\n","5   6  25.0        adult\n","6   7  30.0        adult\n","7   8  28.0        adult\n","8   9  27.0        adult\n","9  10  20.0  young adult\n"]}]},{"cell_type":"markdown","metadata":{"id":"hyUqdihHIv5x"},"source":["Sklearn provides a similar preprocessing utility class for binning, named KBinsDiscretizer. It has the familiar fit and transform API. It also requires that the attribute has no missing values."]},{"cell_type":"code","metadata":{"id":"8ZNhNBUQIykU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634691208853,"user_tz":300,"elapsed":252,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}},"outputId":"a8363483-1438-4e6a-b19e-67774999267d"},"source":["def bin_demo2(df):\n","  # uniform bins\n","  from sklearn.preprocessing import KBinsDiscretizer\n","\n","  # this must be done first\n","  df['age'].fillna(df['age'].mean(), inplace=True)\n","  \n","  binner = KBinsDiscretizer(n_bins=8, encode='ordinal', strategy='uniform')\n","  values = binner.fit_transform(df['age'].values.reshape(-1, 1))\n","  df['age_cat2'] = values\n","  print(df['id age age_cat age_cat2'.split()].head(10))\n","\n","bin_demo2(df_bin.copy())"],"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["   id   age      age_cat  age_cat2\n","0   1  42.0       middle       4.0\n","1   2  13.0        youth       1.0\n","2   3  16.0  young adult       1.0\n","3   4  39.0       middle       4.0\n","4   5  16.0  young adult       1.0\n","5   6  25.0        adult       2.0\n","6   7  30.0        adult       3.0\n","7   8  28.0        adult       3.0\n","8   9  27.0        adult       2.0\n","9  10  20.0  young adult       2.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"l4ftDznMI0hj"},"source":["We can even use the same class to create one-hot encoded bins. By changing it's strategy. Be sure to [read](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) its documentation."]},{"cell_type":"code","metadata":{"id":"qF8ie7FoI24I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634691216792,"user_tz":300,"elapsed":242,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}},"outputId":"fb4d5701-7eb3-4f4f-85b4-fb08600e8782"},"source":["def bin_demo3(df):\n","  # one hot binning\n","  from sklearn.preprocessing import KBinsDiscretizer\n","\n","  bin_count = 4\n","  df['age'].fillna(df['age'].mean(), inplace=True)\n","  binner = KBinsDiscretizer(n_bins=bin_count, encode='onehot-dense', strategy='uniform')\n","  values = binner.fit_transform(df['age'].values.reshape(-1, 1))\n","  labels = ['bin {:d}'.format(i) for i in range(1, bin_count+1)]\n","  \n","  df2 = pd.DataFrame(values, columns=labels)\n","  df2['age'] = df['age']\n","  print(df2.head(10))\n","\n","bin_demo3(df.copy())"],"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["   bin 1  bin 2  bin 3  bin 4   age\n","0    0.0    0.0    1.0    0.0  42.0\n","1    1.0    0.0    0.0    0.0  13.0\n","2    1.0    0.0    0.0    0.0  16.0\n","3    0.0    0.0    1.0    0.0  39.0\n","4    1.0    0.0    0.0    0.0  16.0\n","5    0.0    1.0    0.0    0.0  25.0\n","6    0.0    1.0    0.0    0.0  30.0\n","7    0.0    1.0    0.0    0.0  28.0\n","8    0.0    1.0    0.0    0.0  27.0\n","9    0.0    1.0    0.0    0.0  20.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"hH5CsyNZI5iH"},"source":["#**Numeric Data**\n","\n","The goal for normalizing numeric data is essentially same: you want your values to accurately represent the underlying measurement. However, there's an additional consideration called scaling (or feature scaling). The idea is that you don't want the units of one feature (i.e. column/attribute) to overshadow the units of another.\n","\n","![](https://drive.google.com/uc?export=view&id=1eFOO24Rb9gU98yH43smcy3WxxOvd41Oi)"]},{"cell_type":"markdown","metadata":{"id":"eZ0BwxEZJCD0"},"source":["#**Feature Scaling Data**\n","One of the most important transformation to make is to ensure each of your numeric attributes are scaled so that one attribute's units are in the same 'scale' as others. Feature scaling standardizes the value range of features of the data.\n","\n","Let's go over some vocabulary relevant for feature 'scaling':\n","* **Rescaling** a feature vector (think columns of data) means to add or subtract a constant and then multiply or divide by a constant, as you would do to change the units of measurement of the data, for example, to convert a temperature from Celsius to Fahrenheit.\n","\n","* **Normalizing** a feature vector usually means dividing each element by the norm (L1 or L2) of the vector (||x||). However, for this lesson it will refer to a type of rescaling.\n","\n","* **Standardizing** a vector means subtracting a measure of location (mean or median) and dividing by a measure of scale (e.g. standard deviation)."]},{"cell_type":"markdown","metadata":{"id":"U62td42YJVui"},"source":["**Min-Max Scaling**\n","\n","**(a.k.a Min-Max Normalization)**\n","\n","In min-max scaling, you transform the data such that the features are within a specific range usually [0, 1].\n","\n","Scaling is important for algorithms where distance between data points is important. You want to avoid attributes working together but are on different scales. For example, having one attribute measured in feet while the other is in pounds while another is in miles, will wreak havoc on the ML algorithm's calculations.\n","\n","![](https://drive.google.com/uc?export=view&id=1yjhjzsYIzUuQiL_5tI3g1Rv4E8NUIkzX)\n","\n","\n","You can see how the min-max formula scales by the data range (also called peak-to-peak). Run the code below to view some of the values that are used to scale each data item:"]},{"cell_type":"code","metadata":{"id":"AWE6BKExJh39","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634691222513,"user_tz":300,"elapsed":311,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}},"outputId":"cb837a34-006f-46a4-cf71-8d05ec4e601f"},"source":["import numpy as np\n","def fare_stats(df):\n","  df['fare'].fillna(df['fare'].mean(), inplace=True)\n","\n","  print(df['fare'].min())\n","  min_fair = np.min(df['fare'])\n","  max_fair = np.max(df['fare'])\n","  print(min_fair, max_fair)\n","  print(np.ptp(df['fare']))\n","  \n","df = build_titanic()\n","fare_stats(df.copy())"],"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["total rows 2207\n","3.0305\n","3.0305 512.0607\n","509.0302\n"]}]},{"cell_type":"markdown","metadata":{"id":"sUC9GJyrJkfJ"},"source":["**Exercise**\n","\n","Create a function named fare_min_max_scaled_np that uses NumPy to scale the attributes using the min-max formula given above.\n","\n","* You can only use NumPy to do the calculations.\n","* Add the new values to an attribute named fare_mms \n","* You must return the dataframe"]},{"cell_type":"code","metadata":{"id":"By-jRaozVfyC","executionInfo":{"status":"ok","timestamp":1634691225377,"user_tz":300,"elapsed":319,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}}},"source":["def fare_min_max_scaled_np(df):\n","  df['fare'].fillna(df['fare'].mean(), inplace=True)\n","  min_fair = np.min(df['fare'])\n","  max_fair = np.max(df['fare'])\n","  range = max_fair - min_fair\n","  df['fare_mms'] = (df['fare']-min_fair)/range\n","  return df"],"execution_count":67,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"49lP206MJzDU"},"source":["\n","Be sure to write your own test for fare_min_max_scaled_np function. How will you confirm that the operation succeeded?\n","\n","```\n","def test_fare_mms(df):\n","  pass\n","\n","test_fare_mms(df.copy())\n","```"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ViohSHaiONYa","executionInfo":{"status":"ok","timestamp":1634691228040,"user_tz":300,"elapsed":346,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}},"outputId":"789ef2b3-6b18-4f80-cd9e-2e23fe80846d"},"source":["def test_fare_mms(df):\n","  return print(df['fare_mms'].between(0,1).all())\n","\n","df = build_titanic()\n","df = fare_min_max_scaled_np(df.copy())\n","\n","test_fare_mms(df.copy())"],"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["total rows 2207\n","True\n"]}]},{"cell_type":"markdown","metadata":{"id":"22pIGz5OJ4T_"},"source":["**Sklearn's Min-Max Scalar**\n","\n","Sklearn also provides a simple minmax_scale function. We can use it to confirm our results as well. Below also shows how sklearn uses the fit & transform methods on its MinMaxScaler. In this case, fit would calculate the min and max first; transform then applies the formula to all the data."]},{"cell_type":"code","metadata":{"id":"Cpq9irT9KABJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634691230913,"user_tz":300,"elapsed":247,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}},"outputId":"87a4b712-41e3-4241-9b53-d729d61d4ccc"},"source":["from sklearn.preprocessing import minmax_scale\n","from sklearn.preprocessing import MinMaxScaler\n","\n","def fare_min_max_scaled(df):\n","  df['fare_scaled'] = minmax_scale(df['fare'])\n","\n","  # fit & transform way\n","  scaler = MinMaxScaler()\n","  s_values = scaler.fit_transform(df['fare'].values.reshape(-1,1))\n","  df['fare_scaled2'] = s_values\n","  print(df['fare fare_scaled fare_scaled2'.split()].head(10))\n","\n","fare_min_max_scaled(df.copy())"],"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["      fare  fare_scaled  fare_scaled2\n","0   7.1100     0.008014      0.008014\n","1  20.0500     0.033435      0.033435\n","2  20.0500     0.033435      0.033435\n","3  20.0500     0.033435      0.033435\n","4   7.1300     0.008054      0.008054\n","5   7.1300     0.008054      0.008054\n","6  24.0000     0.041195      0.041195\n","7  24.0000     0.041195      0.041195\n","8  18.1509     0.029704      0.029704\n","9   7.1806     0.008153      0.008153\n"]}]},{"cell_type":"markdown","metadata":{"id":"PROjJb8lKDY1"},"source":["**Mean Normalization Mean Centering**\n","\n","Another version of Min-Max Scaling is named mean normalization. In this case, the mean is subtracted from each value (rather than the minimum value).\n","\n","Note that Mean centering is the subtraction part -- and it can be done without any further scaling. The dividing by the data range provides the scaling part."]},{"cell_type":"markdown","metadata":{"id":"mJzUIZ6VKOUD"},"source":["**Z-Score Scaling; Standardization**\n","\n","Sometimes it's important that an attribute has certain statistical requirements. We can 'normalize' (here normalize is used in the statistical sense) the values such that the data is centered at 0 with a standard deviation of 1. This technique is called z-score scaling or just standardization or z-score normalization.\n","\n","Z-score scaling is done using the following formula:\n","\n","![](https://drive.google.com/uc?export=view&id=1p2OVmwJvXRr-YTkxiOEtmDu3rHLRTngr)\n","\n","Here is the original feature vector, is the mean of that feature vector, and σ is its standard deviation. By subtracting the mean from the distribution, you are essentially shifting it towards left or right by amount equal to mean. By dividing by the standard deviation σ, you are changing the shape of distribution.\n","\n","The new standard deviation of this standardized distribution is 1 and μ = 0.\n","With sklearn, you can do this transformation using the scale function or the StandardScaler class:"]},{"cell_type":"code","metadata":{"id":"fYeeWZLeKW9k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634691235296,"user_tz":300,"elapsed":289,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}},"outputId":"a7f7ab0c-8cbc-4ed0-a0fb-5e41fa2663e1"},"source":["from sklearn.preprocessing import scale\n","from sklearn.preprocessing import StandardScaler\n","\n","def fare_z_scaled(df):\n","\n","  # simple way\n","  df['fare_z'] = scale(df['fare'])\n","\n","  # fit & transform way\n","  scaler = StandardScaler()\n","  s_values = scaler.fit_transform(df['fare'].values.reshape(-1,1))\n","  df['fare_z2'] = s_values\n","  print(df['fare fare_z fare_z2'.split()].head(10))\n","\n","fare_z_scaled(df.copy())"],"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["      fare    fare_z   fare_z2\n","0   7.1100 -0.658679 -0.658679\n","1  20.0500 -0.334534 -0.334534\n","2  20.0500 -0.334534 -0.334534\n","3  20.0500 -0.334534 -0.334534\n","4   7.1300 -0.658178 -0.658178\n","5   7.1300 -0.658178 -0.658178\n","6  24.0000 -0.235588 -0.235588\n","7  24.0000 -0.235588 -0.235588\n","8  18.1509 -0.382107 -0.382107\n","9   7.1806 -0.656911 -0.656911\n"]}]},{"cell_type":"markdown","metadata":{"id":"VRuWiKeCKb-D"},"source":["#**Outlier Detection**\n","\n","Having outliers in the dataset, can affect scaling. Although there are machine learning algorithms to help with outlier detection, there are a couple of simple approaches.\n","<br><br>\n","\n","**Removing the outliers**\n","\n","A very quick way to isolate the outliers is to remove those values that are over 2.5 standard deviations away from the rest of the values."]},{"cell_type":"code","metadata":{"id":"pJyy9e-wKZk1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634691238692,"user_tz":300,"elapsed":282,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}},"outputId":"f6244540-c41b-4757-dc3f-c53a279e28ea"},"source":["import numpy as np\n","def find_fare_outliers(df):\n","  df['fare'].fillna(df['fare'].mean(), inplace=True)\n","  data = df['fare'].values.reshape(-1,1)\n","\n","  m = np.mean(data)\n","  s = np.std(data)\n","  \n","  # identify outliers\n","  cut_off = s * 3.5 # pick any number of standard deviations (usually >= 2.0)\n","  lower, upper = m - cut_off, m + cut_off\n","  # identify outliers\n","  outliers = [x for x in data if x < lower or x > upper]\n","  print(\"{:d} outliers: min {:.2f} max {:.2f}\".format(len(outliers), np.min(outliers),\n","np.max(outliers)))\n","  \n","  # remove outliers\n","  # outliers_removed = [x for x in data if x >= lower and x <= upper]\n","\n","find_fare_outliers(df.copy())"],"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["38 outliers: min 211.06 max 512.06\n"]}]},{"cell_type":"markdown","metadata":{"id":"NhMOb8MVKt8X"},"source":["You can actually remove the outliers by uncommenting the last line."]},{"cell_type":"markdown","metadata":{"id":"PDLpURYYKwc2"},"source":["**Robust Scaling**\n","\n","The mean is highly sensitive to outliers. Sklearn's RobustScaler using IQR (interquartile range) to keep all values between the 25th and 75 quartile. It subtracts the column's median and divides by the interquartile range.\n","\n","This scales the data using\n","\n","\n","![](https://drive.google.com/uc?export=view&id=1U06oPdGbrrt9QVjmX27vdNLaa6GNtO6c)\n","\n","RobustScaler can be used when you want to reduce the effects of having many outliers. However, removing unimportant outliers (see above) should also be done."]},{"cell_type":"markdown","metadata":{"id":"aUpbgixxK9hC"},"source":["**Confused ?**\n","\n","It's not necessarily easy to know which technique to use. The best strategy is to evaluate models with the data prepared using different techniques. Sometimes, it's a combination of a few of the methods.\n","\n","There are a few guidelines to help make a first approximation:\n","* if the distribution of the quantity is normal, then it should be standardized,\n","* if the distribution is not normal, the data should be normalized. This applies if the range of quantity values is large (10s, 100s, etc.) or small (0.01, 0.0001).\n","\n","**Min-Max Scaler**\n","\n","* Re-scales to predetermined range [0–1]\n","* Typical neural network algorithm require data that on a 0-1 scale.\n","* Doesn’t change distribution’s center (doesn’t correct skewness)\n","* The distribution of the feature (or any transformations of the feature ) isn’t Gaussian\n","* Feature falls within a bounded interval (sensitive to outliers)\n","\n","**Standard Scaler**\n","* Shifts distribution’s mean to 0 and unit variance\n","* No predetermined range\n","* Best to use on data that is approximately normally distributed clustering, PCA (those that rely on using variance)\n","\n","**Robust Scaler**\n","* 0 mean and unit variance\n","* Use of quartile ranges makes this less sensitive to (a few) outliers • No predetermined range"]},{"cell_type":"markdown","metadata":{"id":"w4yIEtRhLsea"},"source":["**Custom Cleaning**\n","\n","Sometimes, it's necessary to provide cleaning beyond what a library has to offer. Sklearn's FunctionTransformer can be used in these situations. For example, the dataset has an 'sid' field which is actually a string representing the birth year of the person on board the Titanic. If you wanted to do 'math' on that field, a FunctionTransformer could be used to convert that string into a number:\n","\n","The following code (you should implement it) demonstrates how to convert a string field (sid) to an integer. Once that is done, the field can be used like any valid numeric attribute:\n","\n","\n","> **Coder's Log:** You may see references to integer (int for short) or floating point (float for short). These are different ways to represent numbers. A floating point is a number with a decimal point (e.g 123.45) and an integer has no decimal place (whole numbers). Floating point numbers have more precision.\n","\n","```\n","import numpy as np\n","from sklearn.preprocessing import FunctionTransformer\n","\n","def string_to_float(v):\n","  # v is an array of values\n","  return v.astype(np.float)\n","\n","def string_to_int(v):\n","  # v is an array of values\n","  return v.astype(np.int)\n","\n","def clean_sid(df):\n","\n","  # first clean any missing values\n","  mode = df['sid'].mode()[0]\n","  df['sid'].fillna(mode, inplace=True)\n","\n","  # because 'sid' is pandas StringArray, reshape(-1,1) won't work\n","  # print(type(df['sid'].values))\n","\n","  # either of these will work\n","  values = df['sid'].values\n","  # OR\n","  # attribute = ['sid']\n","  # values = df[attribute]\n","  \n","  transformer = FunctionTransformer(string_to_int)\n","  df['sid'] = transformer.fit_transform(values)\n","  return df\n","```\n","\n","The following code cell demonstrates the custom cleaning:"]},{"cell_type":"code","metadata":{"id":"JEhrRZLImg5a","executionInfo":{"status":"ok","timestamp":1634691246117,"user_tz":300,"elapsed":243,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}}},"source":["import numpy as np\n","from sklearn.preprocessing import FunctionTransformer\n","\n","def string_to_float(v):\n","  # v is an array of values\n","  return v.astype(np.float)\n","\n","def string_to_int(v):\n","  # v is an array of values\n","  return v.astype(np.int)\n","\n","def clean_sid(df):\n","\n","  # first clean any missing values\n","  mode = df['sid'].mode()[0]\n","  df['sid'].fillna(mode, inplace=True)\n","\n","  # because 'sid' is pandas StringArray, reshape(-1,1) won't work\n","  # print(type(df['sid'].values))\n","\n","  # either of these will work\n","  values = df['sid'].values\n","  # OR\n","  # attribute = ['sid']\n","  # values = df[attribute]\n","\n","  transformer = FunctionTransformer(string_to_int)\n","  df['sid'] = transformer.fit_transform(values)\n","  return df"],"execution_count":72,"outputs":[]},{"cell_type":"code","metadata":{"id":"TJkIpExcXt20","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634691249098,"user_tz":300,"elapsed":246,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}},"outputId":"c7de6f3a-3715-4d66-9d7c-bf37fa756c69"},"source":["\n","def demo_custom_cleaning(df):\n","  try:\n","    # this will not work\n","    df['sid'] = df['sid'] - 1912\n","  except Exception as e:\n","    print('invalid math')\n","\n","  # clean it so we can do math on it\n","  df_c = clean_sid(df)\n","\n","  df_c['sid'] = 1912 - df_c['sid']\n","  print(df_c.head())\n","  \n","demo_custom_cleaning(df.copy())"],"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["invalid math\n","   id                            name  gender   age class embarked        country  ticketno   fare  sibsp  parch survived  sid  fare_mms\n","0   1             Abbing, Mr. Anthony    male  42.0   3rd        S  United States    5547.0   7.11    0.0    0.0       no   42  0.008014\n","1   2       Abbott, Mr. Eugene Joseph    male  13.0   3rd        S  United States    2673.0  20.05    0.0    2.0       no   13  0.033435\n","2   3     Abbott, Mr. Rossmore Edward    male  16.0   3rd        S  United States    2673.0  20.05    1.0    1.0       no   16  0.033435\n","3   4  Abbott, Mrs. Rhoda Mary 'Rosa'  female  39.0   3rd        S        England    2673.0  20.05    1.0    1.0      yes   39  0.033435\n","4   5     Abelseth, Miss. Karen Marie  female  16.0   3rd        S         Norway  348125.0   7.13    0.0    0.0      yes   16  0.008054\n"]}]},{"cell_type":"markdown","metadata":{"id":"zycEktuXMJ5m"},"source":["**Pandas Coercion**\n","\n","Although sklearn's FunctionTransformer is incredibly flexible, for simple type conversion (string-to-int, int-to-string, etc), you can use panda's to_numeric method. If any issues happen in the coercion, np.nan will be used:\n","\n","```\n","values = pd.to_numeric(df['xyz_column'], errors='coerce')\n","```\n","\n","The to_numeric method will convert values to floats (real numbers). If you want the conversion to be a specific type (like integer) you can either use the downcast [flag](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html) or the astype method:\n","\n","```\n","values = df['xyz'].astype(int) # if any issues, an exception is thrown\n","values = df['xyz'].astype(int, errors='ignore') # ignore those exceptions\n","```"]},{"cell_type":"markdown","metadata":{"id":"I7GoNnV_Ms7l"},"source":["**Date and Time Care**\n","\n","Handling date and time attributes is a mix of working with text, categorical, and numerical. How you handle the attribute just depends on the level of granularity and its purpose. For example, if you are just marking the month of a purchase, treating dates at the 'month' level as a category is perfectly reasonable. However, if you are working with timestamps of events, then these become numeric fields that you need to work with.\n","\n","Deciding to scale a date/time field is also application specific. If the year of when a car was manufactured becomes important, than that field can be treated as an integer. Another issue is that more recent years will have a higher weight (the scaled number would be closer to 1) than those that happened early on. This may or may not be what you want."]},{"cell_type":"markdown","metadata":{"id":"B1bkxn0aM42D"},"source":["**What is Regularization -- it sounds like normalization?**\n","\n","Regularization has to do with preventing overfitting. They are the techniques used to reduce the error by fitting a function appropriately on the given training set and avoid overfitting.\n","\n","Regularization can be controlled via hyperparameters (those values that are given to configure the algorithm to build a model. Going into the specifics of regularization would be difficult here since it's more appropriate to discuss this when we are trying to avoid over- fitting by adjusting the loss (or objective function). You may hear about L1/L2 regularization which uses the same 'math' as L1, L2 normalization.\n","\n","As a reminder, Overfitting is when the model doesn't generalize the 'pattern' being learned, but 'memorizes' it instead. Regularization attempts to prevent models from overfitting by using a hyperparameter to affecting the parameters or weights the model is learning."]},{"cell_type":"markdown","metadata":{"id":"hcUptRZtNBXo"},"source":["#**Lesson Assignment**\n","**Cleaning Cars**\n","\n","![](https://drive.google.com/uc?export=view&id=1A8hVea1Nx8Ezc4VvFmGx8sMT_xg8ng07)\n","\n","This lesson will be using a classic dataset on automobiles. The origin of this dataset can be found [here](http://lib.stat.cmu.edu/datasets/).\n","\n","However, before we can use it for any machine learning algorithms, it needs a lot of cleaning. We will guide you through all the steps that need to be done. Be sure to print out the dataset (df.head()) so you can first familiarize yourself with the data.\n","\n","For all questions you can solve by using the information in this lesson and (if necessary) the official pandas. (e.g. see [replace](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html)).\n"]},{"cell_type":"markdown","metadata":{"id":"C7ZB_3ZzNPvu"},"source":["**Build the cars**\n","\n","Create a function named build_cars that does the following:\n","* loads 'cars.csv' into a pandas dataframe \n","* returns the dataframe\n","\n","\n","\n","```\n","def build_cars():\n","   # return the pandas dataframe with cars.csv loaded\n","   return None\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"GCNTey-HNdqr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634691255793,"user_tz":300,"elapsed":305,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}},"outputId":"16340939-710c-4dd2-ce03-47fb962017e4"},"source":["import numpy as np\n","import pandas as pd\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.width', 200)\n","\n","def build_cars():\n","  df = pd.read_csv('cars.csv')\n","  print('total rows', len(df))\n","  return df.copy()\n","\n","df = build_cars()"],"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["total rows 262\n"]}]},{"cell_type":"markdown","metadata":{"id":"PqD0TCu1Nie9"},"source":["**Clean Columns**\n","\n","Now do the following to the dataframe\n","\n","* removes the notes and id columns\n","* remove all spaces from column names\n","* replaces all empty string values with np.nan\n","   * see DataFrame.replace method for one possible option\n","* You can use the df.rename or df.columns.str (which allows you access to all the string methods).\n","\n"]},{"cell_type":"code","metadata":{"id":"h2NSo-H-efeT","executionInfo":{"status":"ok","timestamp":1634691259578,"user_tz":300,"elapsed":303,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}}},"source":["def clean_columns(df):\n","  df = df[['mpg', ' cylinders', ' cubicinches', ' hp', ' weightlbs', ' time-to-60', ' year', ' brand']]\n","  df.columns = df.columns.str.replace(' ', '')\n","  df = df.replace(r'^\\s*$', np.NaN, regex=True)\n","  return df\n","\n","df = clean_columns(df)"],"execution_count":75,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HRwqQqK5OFgL"},"source":["##**Clean nan values**\n","\n","For the following columns replace all empty/missing values with the requested value \n","\n","**clean the mpg field**\n","\n","Create a function named clean_mpg\n","* replaces missing values with the median value"]},{"cell_type":"code","metadata":{"id":"blhuxIWTawaJ","executionInfo":{"status":"ok","timestamp":1634691264015,"user_tz":300,"elapsed":246,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}}},"source":["def clean_mpg(df):\n","  df['mpg'].fillna(df['mpg'].median(), inplace=True)\n","  return df\n","\n","df = clean_mpg(df)\n","  "],"execution_count":76,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h0DWik3GOUYD"},"source":["**clean the time-to-60 field** \n","\n","Create a function named clean_t60\n","* replaces missing values with the median value"]},{"cell_type":"code","metadata":{"id":"MEaj6o0LbBPe","executionInfo":{"status":"ok","timestamp":1634691267648,"user_tz":300,"elapsed":381,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}}},"source":["def clean_t60(df):\n","  df['time-to-60'].fillna(df['time-to-60'].median(), inplace=True)\n","  return df\n","\n","df = clean_t60(df)"],"execution_count":77,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G4TbodxmOhRh"},"source":["**clean the year field**\n","\n","Create a function named clean_year\n","\n","* replaces missing values with the mode \n","* converts the field to an integer value"]},{"cell_type":"code","metadata":{"id":"ycl_sOdpbMJg","executionInfo":{"status":"ok","timestamp":1634691271569,"user_tz":300,"elapsed":246,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}}},"source":["def clean_year(df):\n","  mode = df['year'].mode()[0]\n","  df['year'].fillna(mode, inplace=True)\n","  values = df['year'].values\n","  transformer = FunctionTransformer(string_to_int)\n","  df['year'] = transformer.fit_transform(values)\n","  return df\n","\n","df = clean_year(df)"],"execution_count":78,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qyu3EwsQOrHB"},"source":["**clean the cubicinches field**\n","\n","Create a function named clean_ci\n","* replaces missing values with the mode\n","* converts the field to an integer value by using the FunctionTransformer"]},{"cell_type":"code","metadata":{"id":"pcw0fSRYbSHE","executionInfo":{"status":"ok","timestamp":1634691275578,"user_tz":300,"elapsed":242,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}}},"source":["def clean_ci(df):\n","  mode = df['cubicinches'].mode()[0]\n","  df['cubicinches'].fillna(mode, inplace=True)\n","  values = df['cubicinches'].values\n","  transformer = FunctionTransformer(string_to_int)\n","  df['cubicinches'] = transformer.fit_transform(values)\n","  return df\n","\n","df = clean_ci(df)"],"execution_count":79,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g5bPuAU0O9at"},"source":["**clean the weightlbs field**\n","\n","Create a function named clean_wlb\n","* replaces missing values with the mean\n"]},{"cell_type":"code","metadata":{"id":"_0Pv3kyKbkJM","executionInfo":{"status":"ok","timestamp":1634691278309,"user_tz":300,"elapsed":244,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}}},"source":["def clean_wlb(df):\n","  values = df['weightlbs'].values\n","  transformer = FunctionTransformer(string_to_float)\n","  df['weightlbs'] = transformer.fit_transform(values)\n","  df['weightlbs'].fillna(df['weightlbs'].mean(), inplace=True)\n","  return df\n","\n","df = clean_wlb(df)"],"execution_count":80,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X9GiYLk-PMm5"},"source":["**clean the brand field**\n","\n","Create a function named clean_brand\n","* remap this categorical field: \n","* US becomes 0\n","* Europe becomes 1 \n","* Japan becomes 2\n","* place the new value in the field manf"]},{"cell_type":"code","metadata":{"id":"uhp32-n0byPK","executionInfo":{"status":"ok","timestamp":1634691284163,"user_tz":300,"elapsed":460,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}}},"source":["def clean_brand(df):\n","  df['brand'].fillna('unknown', inplace=True)\n","  ord_map = {' Japan.':2, ' US.':0, ' Europe.':1}\n","  df['manf'] = df['brand'].map(ord_map)\n","  return df\n","\n","df = clean_brand(df)\n"],"execution_count":81,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DqD1YwUyPtyN"},"source":["###**Scaling Features**\n","\n","Create a function named scale_features that transforms a list of column names to [0,1] using sklearn's min-max scaling.\n","* return a new dataframe with the scaled columns \n","\n","For example, this code\n","\n","```\n","df_sub = scale_features(cars_df, ['mpg', 'hp])\n","print(df_sub.head())\n","```\n","\n","would generate this output (before and after)\n","\n","```\n","    mpg   hp\n","0  22.0   69\n","1  16.0  100\n","2  14.0  165\n","3  31.9   71\n","4  17.0  140\n","        mpg        hp\n","0  0.327869  0.125000\n","1  0.163934  0.293478\n","2  0.109290  0.646739\n","3  0.598361  0.135870\n","4  0.191257  0.510870\n","```"]},{"cell_type":"code","metadata":{"id":"oOHCzyHNcYLh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634691288459,"user_tz":300,"elapsed":308,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}},"outputId":"95bd22e2-5de0-42de-b980-a5db09615b1f"},"source":["\n","def scale_features(df, feature_list):\n","    for column in feature_list:\n","        df[column] = minmax_scale(df[column])\n","    return df\n","\n","df_sub = scale_features(df, ['mpg', 'hp'])\n","print(df_sub.head())"],"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["        mpg  cylinders  cubicinches        hp  weightlbs  time-to-60  year     brand  manf\n","0  0.327869          4           97  0.125000     2050.0           6  1985    Japan.     2\n","1  0.163934          6          250  0.293478     3278.0          18  1974       US.     0\n","2  0.109290          8          350  0.646739     4209.0          12  1972       US.     0\n","3  0.598361          4           89  0.135870     1925.0          14  1980   Europe.     1\n","4  0.191257          8          302  0.510870     3449.0          11  1971       US.     0\n"]}]},{"cell_type":"markdown","metadata":{"id":"DmClQzvLQHVU"},"source":["###**Flip Features**\n","\n","For the 'time-to-60' a lower value is 'better'. We can remap this column so those cars with fast 'time-to-60' values (low numbers) are close to 1 and those with slow 'time-to-60' values (high numbers) are close to 0. We can simply invert these values.\n","\n","Create a function called flip_features that takes a dataframe list of column names that inverts the values. For example, for the following code:\n","\n","```\n","dfs = scale_features(dfc.copy(), ['time-to-60'])\n","print(dfs.head(5))\n","print(flip_features(dfs.copy(), ['time-to-60']).head(5))\n","```\n","\n","The output would look like this:\n","\n","```\n","   time-to-60  (OLD)\n","0    0.000000\n","1    0.631579\n","2    0.315789\n","3    0.421053\n","4    0.263158\n","   time-to-60  (Flipped)\n","0    1.000000\n","1    0.368421\n","2    0.684211\n","3    0.578947\n","4    0.736842\n","```"]},{"cell_type":"code","metadata":{"id":"E4fPfRudc56W","executionInfo":{"status":"ok","timestamp":1634691301263,"user_tz":300,"elapsed":340,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}}},"source":["def flip_features(df, features):\n","  for column in features:\n","    df[column] = 1 - df[column]\n","  return df"],"execution_count":83,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1DBUNSrkKo5i","executionInfo":{"status":"ok","timestamp":1634691303936,"user_tz":300,"elapsed":254,"user":{"displayName":"PRATYUSH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64","userId":"17344282814232432823"}},"outputId":"f402854a-0936-4586-9d5e-94cc309e68cd"},"source":["dfs = scale_features(df.copy(), ['time-to-60'])\n","\n","print(dfs.head(5))\n","\n","print(flip_features(dfs.copy(), ['time-to-60']).head(5))"],"execution_count":84,"outputs":[{"output_type":"stream","name":"stdout","text":["        mpg  cylinders  cubicinches        hp  weightlbs  time-to-60  year     brand  manf\n","0  0.327869          4           97  0.125000     2050.0    0.000000  1985    Japan.     2\n","1  0.163934          6          250  0.293478     3278.0    0.631579  1974       US.     0\n","2  0.109290          8          350  0.646739     4209.0    0.315789  1972       US.     0\n","3  0.598361          4           89  0.135870     1925.0    0.421053  1980   Europe.     1\n","4  0.191257          8          302  0.510870     3449.0    0.263158  1971       US.     0\n","        mpg  cylinders  cubicinches        hp  weightlbs  time-to-60  year     brand  manf\n","0  0.327869          4           97  0.125000     2050.0    1.000000  1985    Japan.     2\n","1  0.163934          6          250  0.293478     3278.0    0.368421  1974       US.     0\n","2  0.109290          8          350  0.646739     4209.0    0.684211  1972       US.     0\n","3  0.598361          4           89  0.135870     1925.0    0.578947  1980   Europe.     1\n","4  0.191257          8          302  0.510870     3449.0    0.736842  1971       US.     0\n"]}]},{"cell_type":"markdown","metadata":{"id":"gHHtBXaydJId"},"source":["**Steps to submit your work:**\n","\n","\n","1.   Download the notebook from Moodle. It is recommended that you use Google Colab to work on it.\n","2.   Upload any supporting files using file upload option within Google Colab.\n","3.   Complete the exercises and/or assignments\n","4.   Download as .ipynb\n","5.   Name the file as \"lastname_firstname_WeekNumber.ipynb\"\n","6.   After following the above steps, submit the final file in Moodle\n","\n","\n","\n","\n","\n","<h1><center>The End!</center></h1>"]}]}