{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fH7WDNgRmDBc"
   },
   "source": [
    "#**Word2Vec**\n",
    "The previous lesson on word embeddings introduced the idea of creating vectors from words. With these vectors we are able to 'math' on words and show relationships. More importantly, these word embeddings can be used as input into other ML algorithms.\n",
    "\n",
    "This lesson is all about how to create those vectors from text as well as how to use an already trained word embedding model.  \n",
    "\n",
    "#**What is word2vec?**\n",
    "\n",
    "Word2Vec is a machine learning algorithm developed in 2013. Specifically, it uses a neural network (2 layers) and is trained with unlabeled data. Since the data is raw text, there is no 'label' to work with, it's an unsupervised ML technique. You can read an overview and the [paper](https://arxiv.org/pdf/1301.3781.pdf).\n",
    "\n",
    "The open source project (https://github.com/tmikolov/word2vec) is written in C; however, the Python gensim (pronounced jen-sim) library provides a Python implementation.\n",
    "\n",
    "For this lesson, we are going to use a dataset from Kaggle regarding the make and model for cars. It is already included in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1524,
     "status": "ok",
     "timestamp": 1634093925021,
     "user": {
      "displayName": "PRATYUSH KUMAR",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64",
      "userId": "17344282814232432823"
     },
     "user_tz": 300
    },
    "id": "oF9H_TtFml3R",
    "outputId": "bd4bdc3d-1139-4a9a-d56c-6e63546d4496"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['make', 'model', 'year', 'engine', 'fuel', 'type', 'engine', 'hp', 'engine', 'cylinders', 'transmission', 'type', 'driven_wheels', 'number', 'of', 'doors', 'market', 'category', 'vehicle', 'size', 'vehicle', 'style', 'highway', 'mpg', 'city', 'mpg', 'popularity', 'msrp']\n",
      "['bmw', 'series', 'premium', 'unleaded', 'required', 'manual', 'rear', 'wheel', 'drive', 'luxury', 'compact', 'convertible']\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "def build_dataset_raw():\n",
    "  # here's an example of how to use a zipped (compressed) file\n",
    "  filename = 'cars.csv.gz'\n",
    "  # https://www.kaggle.com/CooperUnion/cardataset?select=data.csv\n",
    "  file = gzip.open(filename, 'rb')\n",
    "  # clean and tokenize the text\n",
    "  return [gensim.utils.simple_preprocess(line) for line in file]\n",
    "  \n",
    "def test_raw():\n",
    "  document = build_dataset_raw()\n",
    "  print(document[0])  # make note of the column names\n",
    "  print(document[10]) # row '9'\n",
    "test_raw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YxlM3uxoEak"
   },
   "source": [
    "The above example shows how you open a compressed file in Python. Many data sets are very large and may only be available in a compressed format. The example also shows how you can use gensim to process data as well. You can [read](https://radimrehurek.com/gensim/utils.html#gensim.utils.simple_preprocess) about the gensim API (how to use the methods and functions) for pre-processing as well.\n",
    "\n",
    "Instead of using the raw data, let's make use of Pandas to help us clean the data. Take note of the compression parameter in read_csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1393,
     "status": "ok",
     "timestamp": 1634093929498,
     "user": {
      "displayName": "PRATYUSH KUMAR",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64",
      "userId": "17344282814232432823"
     },
     "user_tz": 300
    },
    "id": "YDvGHVVmfGz1",
    "outputId": "95c45ee1-e903-44e1-8e86-f5a368816e4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Factory Tuner', 'Luxury', 'High-Performance', 'BMW 1 Series M', 'Compact']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def build_dataset():\n",
    "\n",
    "  # another way to read compressed data\n",
    "  filename = 'cars.csv.gz'\n",
    "  df = pd.read_csv(filename, compression='gzip')\n",
    "\n",
    "  # feature selection\n",
    "  # select the fields we want to train word2vec on\n",
    "  features = ['Make', 'Model','Market Category','Vehicle Size','Vehicle Style',\n",
    "              'Engine Fuel Type','Transmission Type','Driven_Wheels']\n",
    "  df = df[features]\n",
    "  df['Make_Model'] = df['Make']  + ' ' + df['Model']\n",
    "  features_revised = ['Market Category','Make_Model','Vehicle Size','Vehicle Style',\n",
    "              'Engine Fuel Type','Transmission Type','Driven_Wheels']\n",
    "  df = df[features_revised]\n",
    "  doc = []\n",
    "  for index, row in df.iterrows():\n",
    "    line = [r for v in row.values for r in str(v).split(',')]\n",
    "    doc.append(line)\n",
    "  return doc, df\n",
    "\n",
    "def test_pd_data():\n",
    "  document, df = build_dataset()\n",
    "  print(document[0][0:5])\n",
    "\n",
    "test_pd_data()\n",
    "\n",
    "# ==> ['Factory Tuner', 'Luxury', 'High-Performance', 'Compact', 'Coupe']\n",
    "# ['Factory Tuner', 'Luxury', 'High-Performance', 'BMW 1 Series M', 'Compact']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1636,
     "status": "ok",
     "timestamp": 1634093936603,
     "user": {
      "displayName": "PRATYUSH KUMAR",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64",
      "userId": "17344282814232432823"
     },
     "user_tz": 300
    },
    "id": "mhErfTyyjHT3",
    "outputId": "892312e8-541f-4a21-993a-82513b55b785"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713\n"
     ]
    }
   ],
   "source": [
    "def build_model_v0(doc):\n",
    "  model = gensim.models.Word2Vec(doc)\n",
    "  return model\n",
    "\n",
    "def test_v0():\n",
    "    document, df = build_dataset()\n",
    "    model = build_model_v0(document)\n",
    "    print(len(model.wv.vocab))\n",
    "\n",
    "test_v0()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8-DBK4R46hg"
   },
   "source": [
    "#**Exercise**\n",
    "Update build_dataset such that you will create the field Make_Model in the dataset. Be sure to add this field to the front of each line of the output. The new field Make_Model combines the fields Make and Model with a single space between them.\n",
    "Once this is finished, test_pd_data should print out the following:\n",
    "\n",
    "```\n",
    "['BMW 1 Series M', 'Factory Tuner', 'Luxury', 'High-Performance', 'Compact']\n",
    "```\n",
    "\n",
    "You should also confirm that there are 928 unique Make_Models in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cW_Vy4W1kcOg"
   },
   "source": [
    "#**Model Building**\n",
    "For creating word2vec models, gensim's Word2Vec class is available. It essentially implements the classic algorithm mentioned in the beginning. In the next code cell, re-type in the following:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "def build_model_v0(doc):\n",
    "  model = gensim.models.Word2Vec(doc)\n",
    "  return model\n",
    "\n",
    "def test_v0():\n",
    "    document, df = build_dataset()\n",
    "    model = build_model_v0(document)\n",
    "    print(len(model.wv.vocab))\n",
    "\n",
    "test_v0()\n",
    "```\n",
    "\n",
    "Note that the .wv property of the model is the word vector object that provides access into the word vectors themselves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiO-g2hOlL-h"
   },
   "source": [
    "#**Model Evaluation: Extrinsic vs Intrinsic evaluation**\n",
    "\n",
    "Of course, we have no idea on how 'good' the default Word2Vec function is at building word embeddings. We need a way to evaluate it. For evaluating how useful/accurate the word embeddings are, there are two different ways to assess them: intrinsically and extrinsically.\n",
    "\n",
    "In **intrinsic** evaluation, you are assessing the performance on a very specific task or sub- task for the vectors themselves. For example, one task might be how many word analogies are correctly identified.\n",
    "<br>\n",
    "\n",
    "In **extrinsic** evaluation, you are using your word vectors as input into another NLP process (e.g. named entity recognition, classification, another neural network).\n",
    "\n",
    "For this example, we will evaluate our simple model using a few intrinsic evaluations:\n",
    "1. Do the word vectors capture all the make/models of the car set?\n",
    "2. How accurate are the car similarities? For example, we would expect 'Toyota Camry' and 'Nissan Van' to be closer than 'Toyota Camry' and 'Mercedes-Benz SLK-Class'.\n",
    "\n",
    "Read, understand and run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1681,
     "status": "ok",
     "timestamp": 1634093945665,
     "user": {
      "displayName": "PRATYUSH KUMAR",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64",
      "userId": "17344282814232432823"
     },
     "user_tz": 300
    },
    "id": "ZaFLY23_lwQU",
    "outputId": "ad76814e-ac04-460c-9a21-fc487fe05458"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263 models are missing of 928\n",
      "Toyota Camry->Honda Accord 0.9588\n",
      "\n",
      "Error:\"word 'Nissan Van' not in vocabulary\"\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, df=None):\n",
    "\n",
    "  output = ''\n",
    "  if df is not None:\n",
    "    unique_set = df['Make_Model'].unique()\n",
    "    missing=0\n",
    "    for mm in unique_set:\n",
    "      if mm not in model.wv.vocab:\n",
    "        missing += 1\n",
    "    output += \"{:d} models are missing of {:d}\\n\".format(missing, len(unique_set))\n",
    "\n",
    "  try:\n",
    "    t = 'Toyota Camry'\n",
    "    other = ['Honda Accord', 'Nissan Van', 'Mercedes-Benz SLK-Class']\n",
    "    for o in other:\n",
    "      output += t + '->' + o + ' ' + \"{:0.4f}\\n\".format(model.wv.similarity(t,o))\n",
    "\n",
    "    tuples = model.wv.most_similar(positive='Honda Odyssey', topn=3)\n",
    "    for mm, v in tuples:\n",
    "      output += mm + ', '\n",
    "    output = output.strip(', ')\n",
    "\n",
    "  except KeyError as e:\n",
    "    output += \"\\nError:\" + str(e)\n",
    "\n",
    "  return output\n",
    "\n",
    "def test_v0():\n",
    "  document, df = build_dataset()\n",
    "  model = build_model_v0(document)\n",
    "  print(evaluate_model(model, df))\n",
    "  \n",
    "test_v0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 597,
     "status": "ok",
     "timestamp": 1634093955910,
     "user": {
      "displayName": "PRATYUSH KUMAR",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64",
      "userId": "17344282814232432823"
     },
     "user_tz": 300
    },
    "id": "DBbb-ZG0lSVh"
   },
   "outputs": [],
   "source": [
    "def build_model_v1(doc):\n",
    "  model = gensim.models.Word2Vec(\n",
    "          doc,\n",
    "          min_count=1,\n",
    "          workers=1,\n",
    "          window=10 # only ignore words that occur less than 1 times\n",
    "          )\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2129,
     "status": "ok",
     "timestamp": 1634093961228,
     "user": {
      "displayName": "PRATYUSH KUMAR",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64",
      "userId": "17344282814232432823"
     },
     "user_tz": 300
    },
    "id": "MYoSmnmjlZlj",
    "outputId": "1c5bf51c-e32d-4c71-8b41-6a73e9310770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 models are missing of 928\n",
      "Toyota Camry->Honda Accord 0.9588\n",
      "Toyota Camry->Nissan Van 0.9490\n",
      "Toyota Camry->Mercedes-Benz SLK-Class 0.5253\n",
      "Dodge Caravan, Buick Terraza, Chevrolet Silverado 1500 Hybrid\n"
     ]
    }
   ],
   "source": [
    "def test_v1():\n",
    "  document, df = build_dataset()\n",
    "  model = build_model_v1(document)\n",
    "  print(evaluate_model(model,df))\n",
    "test_v1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 184,
     "status": "ok",
     "timestamp": 1634093965370,
     "user": {
      "displayName": "PRATYUSH KUMAR",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64",
      "userId": "17344282814232432823"
     },
     "user_tz": 300
    },
    "id": "n54XGkgxqy_Z",
    "outputId": "b944ac47-c47b-4268-efe9-457aa98347d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "print(multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BQBhHBRl0l1"
   },
   "source": [
    "What did you notice for test_v0? Of course, this isn't a thorough testing suite; but it helps to show some simple relationships. Ideally, you would come up with score/metric for your evaluation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7R6VUS8l5io"
   },
   "source": [
    "**Tuning Our Algorithm**\n",
    "\n",
    "There's another parameter (many actually) that we can use to configure Word2Vec. These parameters (called hyper parameters) along with our evaluation function can be used to build an accurate model based on our dataset. The values of these hyper-parameters come from experience and trail-and-error.\n",
    "\n",
    "The first parameter is min_count whose default value is 5. There are many car models that only appear a few times and these cars are being dropped. Let's update our model to use this parameter:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "def build_model_v1(doc):\n",
    "  model = gensim.models.Word2Vec(\n",
    "          doc,\n",
    "          min_count=1, # only ignore words that occur less than 1 times\n",
    "          )\n",
    "  return model\n",
    "```\n",
    "add the following code to the above cell (`build_model_v1`) and run `test_v1`\n",
    "```\n",
    "def test_v1():\n",
    "  document, df = build_dataset()\n",
    "  model = build_model_v1(document)\n",
    "  print(evaluate_model(model,df))\n",
    "test_v1()\n",
    "```\n",
    "\n",
    "That's much better (your output will be different, but all the car models should be there):\n",
    "\n",
    "```\n",
    "0 models are missing of 928\n",
    "Toyota Camry->Honda Accord 0.9584\n",
    "Toyota Camry->Nissan Van 0.9442\n",
    "Toyota Camry->Mercedes-Benz SLK-Class 0.6755\n",
    "Toyota Previa, Pontiac Montana, Chevrolet Uplander\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1khsvKWDnDqD"
   },
   "source": [
    "#**Randomness of ML**\n",
    "\n",
    "You may see different numbers in your output than what is shown. Many machine learning algorithms use randomization to make sure things are evenly spaced out in high dimensional space to start. So if you re-run your above model, you should see different results each time -- but on average, your results should be close on each run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoJzXbm7nH5o"
   },
   "source": [
    "**CPUS and Threads**\n",
    "\n",
    "However, this randomness causes issues with reproducibility. We can control the randomness by doing a few things. The main issue for Word2Vec is that the work it does is split across many threads. You can think of a thread as an independent worker. Usually you want to at least match the number of CPUs to the number of threads. That way if you have multiple CPUS, you can take advantage of parallel processing. For this lesson, we will not worry about how to find the number of CPUS our VM has. But you can get this information from within a Python program.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "import multiprocessing\n",
    "print(multiprocessing.cpu_count())\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUbKV7SBnkKc"
   },
   "source": [
    "***Coder's Log***: a process is an active program. It has it's own memory and resources. A thread is 'lightweight' in that it can share the same memory of it's parent process. Processes are isolated; threads are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzy5gP84n7Gu"
   },
   "source": [
    "When work is split up between threads, each thread may be assigned different units of work, finish at different times and their results may be combined differently. At the cost of being less efficient, we can tell Word2Vec to only use a single thread. That will stop the randomness. Note that there is also a seed hyperparameter that can be used to control randomness.\n",
    "\n",
    "\n",
    "Go back to the previous code cell and update your code (and run it):\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "def build_model_v1(doc):\n",
    "  model = gensim.models.Word2Vec(\n",
    "             doc,\n",
    "             min_count=1, # ignore words that occur less than 1 times\n",
    "             workers=1\n",
    "          )\n",
    "  return model\n",
    "  \n",
    "def test_v1():\n",
    "  document, df = build_dataset()\n",
    "  model = build_model_v1(document)\n",
    "  print(evaluate_model(model,df))\n",
    "test_v1()\n",
    "```\n",
    "You should now see consistent numbers between multiple runs. Here's the output we get (your output will be slightly different):\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "0 models are missing of 928\n",
    "Toyota Camry->Honda Accord 0.9773\n",
    "Toyota Camry->Nissan Van 0.9501\n",
    "Toyota Camry->Mercedes-Benz SLK-Class 0.4989\n",
    "GMC Jimmy, Ford Five Hundred, GMC Envoy\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RIRc7kmoluj"
   },
   "source": [
    "#**Windows of Context**\n",
    "The output of word2vec is a set of word vectors. And each word vector is essentially the same as shown in the previous lesson on word embeddings. The goal of the algorithm is to have words with similar context occupy close spatial positions. As discussed in the word embeddings lesson, the cosine similarity can be used as a metric of closeness.\n",
    "\n",
    "For word2vec there is a concept of defining both a 'target word' and 'context words'. Below shows an example of the target word 'by' with its context window (word is known the company it):\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1QR5HR3sFX5Dt1ZDb_sNxoX1xEsf4uMl7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cg_ruFOeqIfG"
   },
   "source": [
    "For the window of size n the contexts are defined by capturing n words to the left of the target and n words to its right. This window of context (shown here to be size 3) slides along the text. So the next word that is processed is the:\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1Ao5X5Z07uZdnZNQRRaRhH9kDdYJ7pPc-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X15M3K9DqN-3"
   },
   "source": [
    "Given that information, it's clear that where the target word appears in the document and the size of the context window can affect the quality of the output. If the window is too small, 'meaning' becomes very narrow. If the window is too big, words no longer separate from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "to7Y2lPRqgrr"
   },
   "source": [
    "###**Exercise**\n",
    "\n",
    "Go all the way back to the code cell that creates the function build_dataset. Move the column 'Make_Model' from the front of the list to the third position. Now Re-run the cell with build_dataset in it. You should see the following (from the output of test_pd_data):\n",
    "\n",
    "```\n",
    "['Factory Tuner', 'Luxury', 'High-Performance', 'BMW 1 Series M', 'Compact']\n",
    "```\n",
    "\n",
    "Now re-run the cell with test_v1():\n",
    "\n",
    "```\n",
    "test_v1()\n",
    "```\n",
    "\n",
    "Notice that the position of where the make/model appears in the document affects the result (the similarity of Camry and Accord went down). We can avoid this issue by creating a wide context window.\n",
    "\n",
    "The default window size is 5. Do the following modifications:\n",
    "* update build_model_v1 to be the following:\n",
    "\n",
    "```\n",
    "def build_model_v1(doc):\n",
    "  model = gensim.models.Word2Vec(\n",
    "             doc,\n",
    "             min_count=1, # ignore words that occur less than 1 times\n",
    "             workers=1,   # one thread to remove randomness\n",
    "             window=10,   # wide window size\n",
    "          )\n",
    "  return model\n",
    "```\n",
    "\n",
    "When you re-run the cell (test_v1()) the output becomes:\n",
    "\n",
    "```\n",
    "0 models are missing of 928\n",
    "Toyota Camry->Honda Accord 0.9734\n",
    "Toyota Camry->Nissan Van 0.9292\n",
    "Toyota Camry->Mercedes-Benz SLK-Class 0.1101\n",
    "Dodge Ramcharger, Pontiac Montana, GMC Jimmy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zl7S0dMCubcT"
   },
   "source": [
    "#**Epoch Training**\n",
    "\n",
    "As we saw in the ML Prep lesson, each machine learning algorithm involves iteration over the dataset to help adjust and improve. Initially, the word vectors are assigned random locations in very high dimensional space. As the algorithm iterates, these word vectors move closer to neighborhoods with 'similar words'.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1v1C8k8v-rZnaDtRVv2nqb0eEwmlZuKqa)\n",
    "\n",
    "Remember, that 'closeness' is defined by\n",
    "how similar these words are. And being similar, means the words share similar contexts. So you expect common misspellings and upper/lower case versions of the same word to be located near each other in high dimensional space. The image to the left shows how the days of the week (orange circle) might be near each other. Also similar relationships would have similar distances (e.g. king to queen and uncle to aunt)\n",
    "\n",
    "\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1Gvt7N27XLaHRlpvYe2qGV732FGVgt24S)\n",
    "\n",
    "\n",
    "You can control how many times word2vec iterates on its training through the iter parameter (whose default is 5). Let's up this to 15. Of course, this is a choice that comes from experimentation and evaluation. If your corpus is huge, you may not have enough years to iterate.\n",
    "\n",
    "\n",
    "Let's create another function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2322,
     "status": "ok",
     "timestamp": 1634094104261,
     "user": {
      "displayName": "PRATYUSH KUMAR",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64",
      "userId": "17344282814232432823"
     },
     "user_tz": 300
    },
    "id": "DAcCOZqJvqEY",
    "outputId": "b91cf416-e77b-40a4-f2a8-5885dd9922bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 models are missing of 928\n",
      "Toyota Camry->Honda Accord 0.8851\n",
      "Toyota Camry->Nissan Van 0.6795\n",
      "Toyota Camry->Mercedes-Benz SLK-Class 0.0497\n",
      "Dodge Caravan, Toyota Sienna, Plymouth Grand Voyager\n"
     ]
    }
   ],
   "source": [
    "def build_model_v2(doc):\n",
    "  model = gensim.models.Word2Vec(\n",
    "          doc,\n",
    "          min_count=1,   # ignore words that occur less than 2 times\n",
    "          workers=1,     # threads to use\n",
    "          window=10,     # size of window around the target word\n",
    "          iter=15       # 15 epochs\n",
    "          )\n",
    "  return model\n",
    "\n",
    "\n",
    "def test_v2():\n",
    "  document, df = build_dataset()\n",
    "  model = build_model_v2(document)\n",
    "  print(evaluate_model(model,df))\n",
    "test_v2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3jRJr-Av0g_"
   },
   "source": [
    "The output should look close to the following:\n",
    "\n",
    "```\n",
    "0 models are missing of 928\n",
    "Toyota Camry->Honda Accord 0.8234\n",
    "Toyota Camry->Nissan Van 0.6933\n",
    "Toyota Camry->Mercedes-Benz SLK-Class -0.0498\n",
    "Ford Aerostar, GMC Safari, Dodge Caravan\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EkdV9vGIv738"
   },
   "source": [
    "This looks a lot better. The three similar vans are correct and the Camry and Mercedes have a lot more distance between them (negative in fact). Note: your output will look slightly different. But you should see an improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kc247lKKoLbr"
   },
   "source": [
    "#**High Dimensional Space**\n",
    "\n",
    "As we saw in the word embeddings lesson, word vectors have a length (we saw that spaCy uses 300) that indicates how many dimensions each word contains. The default for word2vec is 100.\n",
    "\n",
    "This is another hyper-parameter that you can adjust. There's no perfect number. The larger your corpus is the more dimensions you will need. The cars dataset is very small so it would be good to know how many dimensions capture the similarities between cars.\n",
    "\n",
    "You want the smallest number of dimensions necessary to do well on your evaluation metrics (and no more). Too many dimensions become space inefficient as your corpus size increase and could result in **overfitting** (when the model doesn't generalize well).\n",
    "\n",
    "Update build_model_v2 to allow the number of dimensions to be passed in.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "def build_model_v2(doc):\n",
    "  model = gensim.models.Word2Vec(\n",
    "          doc,\n",
    "          min_count=1,   # ignore words that occur less than 2 times\n",
    "          workers=1,     # threads to use\n",
    "          window=10,     # size of window around the target word\n",
    "          iter=15        # 15 epochs\n",
    "          size=ndim, # how big the output vectors (spacy == 300)\n",
    "          )\n",
    "  return model\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlT2DyrYoZM-"
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "Experiment with using 25, 50, 75, 100, 150, 200. Update test_v2 to call build_model_v2 in a loop of the different sizes.\n",
    "\n",
    "What do you notice and where would you decide to put the cutoff?\n",
    "\n",
    "I noticed that using 125 as number of dimensions works better than other values in the list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13246,
     "status": "ok",
     "timestamp": 1634094086460,
     "user": {
      "displayName": "PRATYUSH KUMAR",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64",
      "userId": "17344282814232432823"
     },
     "user_tz": 300
    },
    "id": "WE3gk0DP7NsY",
    "outputId": "5ad2d27a-9c4c-4ca5-a2d4-1f8013268733"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 models are missing of 928\n",
      "Toyota Camry->Honda Accord 0.8781\n",
      "Toyota Camry->Nissan Van 0.6407\n",
      "Toyota Camry->Mercedes-Benz SLK-Class -0.0275\n",
      "Dodge Caravan, Plymouth Grand Voyager, Toyota Sienna\n",
      "0 models are missing of 928\n",
      "Toyota Camry->Honda Accord 0.8902\n",
      "Toyota Camry->Nissan Van 0.6552\n",
      "Toyota Camry->Mercedes-Benz SLK-Class -0.0060\n",
      "Dodge Caravan, Plymouth Grand Voyager, Toyota Sienna\n",
      "0 models are missing of 928\n",
      "Toyota Camry->Honda Accord 0.8896\n",
      "Toyota Camry->Nissan Van 0.6684\n",
      "Toyota Camry->Mercedes-Benz SLK-Class 0.0358\n",
      "Dodge Caravan, Plymouth Grand Voyager, Toyota Sienna\n",
      "0 models are missing of 928\n",
      "Toyota Camry->Honda Accord 0.8851\n",
      "Toyota Camry->Nissan Van 0.6795\n",
      "Toyota Camry->Mercedes-Benz SLK-Class 0.0497\n",
      "Dodge Caravan, Toyota Sienna, Plymouth Grand Voyager\n",
      "0 models are missing of 928\n",
      "Toyota Camry->Honda Accord 0.8826\n",
      "Toyota Camry->Nissan Van 0.6919\n",
      "Toyota Camry->Mercedes-Benz SLK-Class 0.0768\n",
      "Dodge Caravan, Toyota Sienna, Volkswagen Routan\n",
      "0 models are missing of 928\n",
      "Toyota Camry->Honda Accord 0.8818\n",
      "Toyota Camry->Nissan Van 0.7021\n",
      "Toyota Camry->Mercedes-Benz SLK-Class 0.0861\n",
      "Dodge Caravan, Volkswagen Routan, Toyota Sienna\n"
     ]
    }
   ],
   "source": [
    "def build_model_v2(doc,ndim):\n",
    "  model = gensim.models.Word2Vec(\n",
    "          doc,\n",
    "          min_count=1,   # ignore words that occur less than 2 times\n",
    "          workers=1,     # threads to use\n",
    "          window=10,     # size of window around the target word\n",
    "          iter=15,        # 15 epochs\n",
    "          size=ndim # how big the output vectors (spacy == 300)\n",
    "          )\n",
    "  return model\n",
    "\n",
    "def test_v2(sizes):\n",
    "  for size in sizes:\n",
    "    document, df = build_dataset()\n",
    "    model = build_model_v2(document,size)\n",
    "    print(evaluate_model(model,df), sep='')\n",
    "\n",
    "test_v2([25, 50, 75, 100, 150, 200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Med2H7LGU90e"
   },
   "source": [
    "#**Two Ways To Train**\n",
    "\n",
    "Word2Vec uses a neural network as it's algorithm and architecture. We will go into more detail in the lesson on using neural networks. For now, we will simplify things a bit just so we can stay focused on the task at hand.\n",
    "\n",
    "Word2vec provides two very different ways to structure the neural network for learning the distributed representations of words that try to minimize the computational complexity (how long it takes to run). These two underlying architectures are the continuous bag-of-words model (CBOW) and a continuous Skip-gram (Skip Gram) model. Each uses a different metric to evaluate the training of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SciOO3y2VJom"
   },
   "source": [
    "#**Continuous bag-of-words (CBOW) training**\n",
    "\n",
    "In this method, a window of words surrounding the 'target' word (i.e. the context) is used in an attempt to predict the target word.\n",
    "\n",
    "It's a 'bag of words' in that the actual order of the surrounding words is not used in the analysis. It uses a continuous probability distribution to represent the context words (rather than discrete counting).\n",
    "\n",
    "The input into CBOW is a vector representation of a group of context words, the goal is to get the most appropriate target word which will be within the vicinity of the group of words.\n",
    "\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1Csgy5_ICG58GuiOGsojGZo3Fs0cUUBwW)\n",
    "\n",
    "\n",
    "#**Skip-gram training**\n",
    "\n",
    "For CBOW, if you have enough context, the goal is to predict the word. In the skip-gram 'model', if you are given a target word, the output is the set of context words (i.e. words who appeared in close proximity to the target).\n",
    "\n",
    "Essentially the task the neural network is solving is to find which context words can appear given a target word. After training the neural network, if we input any target word into the neural network, it will give a vector output which represents the words which have a high probability of appearing near the given word.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1dkeVAr5maRiSm2jmauQ_f2cpu9dg4hkK)\n",
    "\n",
    "\n",
    "#**Choosing between the two**\n",
    "\n",
    "The author of word2vec [summarizes the differences](https://groups.google.com/g/word2vec-toolkit/c/NLvYXU99cAM/m/E5ld8LcDxlAJ?pli=1) of CBOW and SkipGram:\n",
    "\n",
    "* Skip-gram: works well with small amount of the training data, represents well even rare words or phrases\n",
    "\n",
    "* CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words\n",
    "\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1TVBn_sskOkehevhJfcs9sQvd8uiFAz-w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNsSyJyqWKp5"
   },
   "source": [
    "The default training method of word2vec is CBOW. But since our car dataset is so small, let's try the skipgram model by using the sg named parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3859,
     "status": "ok",
     "timestamp": 1634094120101,
     "user": {
      "displayName": "PRATYUSH KUMAR",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64",
      "userId": "17344282814232432823"
     },
     "user_tz": 300
    },
    "id": "iSNlB1yZXY5u",
    "outputId": "97b6d2c2-0d19-43c0-a6b1-f61babd2e27a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 models are missing of 928\n",
      "Toyota Camry->Honda Accord 0.9111\n",
      "Toyota Camry->Nissan Van 0.8345\n",
      "Toyota Camry->Mercedes-Benz SLK-Class 0.6333\n",
      "GMC Safari, Dodge Caravan, Chevrolet Astro\n"
     ]
    }
   ],
   "source": [
    "def build_model_v3(doc):\n",
    "  model = gensim.models.Word2Vec(\n",
    "          doc,\n",
    "          min_count=1,   # ignore words that occur less than 2 times\n",
    "          workers=1,     # threads to use\n",
    "          window=10,     # size of window around the target word\n",
    "          iter=15,        # 15 epochs\n",
    "          size=100,     # how big the output vectors (spacy == 300)\n",
    "          sg=1,\n",
    "          negative=15          # 0 == CBOW (default) 1 == skip gram\n",
    "          )\n",
    "  return model\n",
    "\n",
    "def test_v3():\n",
    "  document, df = build_dataset()\n",
    "  model = build_model_v3(document)\n",
    "  print(evaluate_model(model,df))\n",
    "  model.save('carmodel.skipgram')\n",
    "\n",
    "test_v3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 117,
     "status": "ok",
     "timestamp": 1634067188325,
     "user": {
      "displayName": "PRATYUSH KUMAR",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghf4mVi7jZSFhHFjQNpiFToq2cNJwDkOu-_CsNdDw=s64",
      "userId": "17344282814232432823"
     },
     "user_tz": 300
    },
    "id": "sRG5bc9g-Nss",
    "outputId": "8d520622-b4da-4108-c878-de72dfe333f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toyota Camry->Honda Accord 0.9063\n",
      "Toyota Camry->Nissan Van 0.8327\n",
      "Toyota Camry->Mercedes-Benz SLK-Class 0.6258\n",
      "Dodge Caravan, GMC Safari, Chevrolet Astro\n"
     ]
    }
   ],
   "source": [
    "def test_load():\n",
    "  md2 = gensim.models.Word2Vec.load('carmodel.skipgram')\n",
    "  print(evaluate_model(md2))\n",
    "\n",
    "test_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ut9jYc1oXhU4"
   },
   "source": [
    "When you run it, you should see something similar to\n",
    "\n",
    "```\n",
    "0 models are missing of 928\n",
    "Toyota Camry->Honda Accord 0.8568\n",
    "Toyota Camry->Nissan Van 0.8036\n",
    "Toyota Camry->Mercedes-Benz SLK-Class 0.5291\n",
    "GMC Safari, Chevrolet Astro, Dodge Caravan\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cS2TvUcjXtOK"
   },
   "source": [
    "In order to evaluate the accuracy between CBOW or skip-gram, we would need a more extensive test suite. But you can see that skip-gram did perform very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0chhQ8-7X12f"
   },
   "source": [
    "#**Negative Sampling**\n",
    "\n",
    "Without getting into the details (they will come), training a neural network (NN) is very time consuming. A NN is made up of connected nodes and layers.\n",
    "\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1lhZc_wHJiH3474H1MRDvnuQdlrywtav7)\n",
    "\n",
    "You can also think of each connection (a line or edge between nodes) as having a 'weight' that needs to be adjusted. As the size of the vocabulary increases (the number of unique words in the corpus) so does the complexity of the internal architecture (i.e. a lot more nodes, edges and weights to adjust).\n",
    "\n",
    "Negative sampling addresses the complexity issue by having each training sample modify only a small percentage of the nodes/weights (rather than all of them). With negative sampling, we randomly select just a small number of “negative” words to update the weights for. In this context, a “negative” word is one for which we want the network to output a 0).\n",
    "\n",
    "Another option for the training method is called soft-max. Soft-max is computational expensive and is usually referred to as hierarchical soft-max which is an optimized implementation. We can cover the details of these algorithms when we get to neural networks.\n",
    "\n",
    "For word2vec, the default negative sampling parameter is set to 5. Update the function build_model_v3 to include 15 to be the value:\n",
    "\n",
    "```\n",
    "negative=15,\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "When you re-run, test_v3, you should see results similar to the following:\n",
    "\n",
    "```\n",
    "0 models are missing of 928\n",
    "Toyota Camry->Honda Accord 0.8977\n",
    "Toyota Camry->Nissan Van 0.8216\n",
    "Toyota Camry->Mercedes-Benz SLK-Class 0.6351\n",
    "GMC Safari, Ford Windstar, Chevrolet Astro\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gv4RAFgYTh4"
   },
   "source": [
    "#**Saving and Loading Models**\n",
    "\n",
    "Once you train a model (which can take hours, days, weeks, months?), you will want to save it to a file so you can just reload the trained model. The model you save is significantly smaller than the corpus you used to train it.\n",
    "\n",
    "For gensim, you can save models via the save method. Update your test_v3 function to include saving the model:\n",
    "\n",
    "**Saving Models**\n",
    "\n",
    "```\n",
    "def test_v3():\n",
    "  document, df = build_dataset()\n",
    "  model = build_model_v3(document)\n",
    "  print(evaluate_model(model,df))\n",
    "  model.save('carmodel.skipgram')\n",
    "test_v3()\n",
    "```\n",
    "\n",
    "**Loading Models**\n",
    "\n",
    "You can reload saved models just as easily:\n",
    "```\n",
    "def test_load():\n",
    "  md2 = gensim.models.Word2Vec.load('carmodel.skipgram')\n",
    "  print(evaluate_model(md2))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puZlOW-Pbdts"
   },
   "source": [
    "**Word Analogies Test Suite**\n",
    "\n",
    "A classic set of word analogies is also available to use (see https://github.com/nicholas-leonard/word2vec/blob/master/questions-words.txt)\n",
    "The word2vec model also provides a way to do the evaluation easily as well:\n",
    "\n",
    "```\n",
    "model.wv.evaluate_word_analogies\n",
    "```\n",
    "\n",
    "See the [documentation](https://radimrehurek.com/gensim/models/keyedvectors.html) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMlRDUQkcIRl"
   },
   "source": [
    "#**fastText (2016)**\n",
    "\n",
    "Facebook's implementation for creating word embeddings and sentence classification is called fastText. It is written in C++ and supports multiprocessing during training. It's word vectors are actually sub-words. You can [read](https://arxiv.org/pdf/1607.04606.pdf) about it here. You can even install the fasttext Python library. The process of installing and evaluating the models will be very straightforward.\n",
    "\n",
    "#**Summary**\n",
    "There's a lot going on in this lesson. So much in fact that the tests for this lesson will only confirm that you wrote the necessary functions. We'll have a separate lesson that allows you to work on a corpus and build your own word embeddings.\n",
    "\n",
    "#**Lesson Assignment**\n",
    "If you followed along with the lesson, you should be good to go. All the model building (and loading) takes too long. But be sure that all the functions are properly written and run without errors. You still need to submit your notebook in Moodle for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_9ToiksMdCs"
   },
   "source": [
    "**Steps to submit your work:**\n",
    "\n",
    "\n",
    "1.   Download the notebook from Moodle. It is recommended that you use Google Colab to work on it.\n",
    "2.   Upload any supporting files using file upload option within Google Colab.\n",
    "3.   Complete the exercises and/or assignments\n",
    "4.   Download as .ipynb\n",
    "5.   Name the file as \"lastname_firstname_WeekNumber.ipynb\"\n",
    "6.   After following the above steps, submit the final file in Moodle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<h1><center>The End!</center></h1>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Kumar_Pratyush_Week7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
